{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69d1f48d21cd2b4",
   "metadata": {
    "id": "c69d1f48d21cd2b4"
   },
   "source": [
    "# FlashRank 重排器\n",
    "## 概述\n",
    "> [FlashRank](https://github.com/PrithivirajDamodaran/FlashRank) 是一個超輕量級和超快速的 Python 函式庫，專為在現有搜尋和**檢索**管道中添加重排序功能而設計。它基於最先進（****SoTA****）的 **cross-encoders**。\n",
    "\n",
    "本筆記介紹在 LangChain 框架中使用 ```FlashRank-Reranker```，展示如何應用重排序技術來改善搜尋或**檢索**結果的品質。它提供了將 ```FlashRank``` 整合到 LangChain 管道的實用程式碼範例和說明，突出其效率和有效性。重點在於利用 ```FlashRank``` 的能力，以簡化且可擴展的方式增強輸出排名。\n",
    "\n",
    "### 目錄\n",
    "- [概述](#概述)\n",
    "- [環境設定](#環境設定)\n",
    "- [FlashRankRerank](#flashrankrerank)\n",
    "\n",
    "---\n",
    "\n",
    "## 我的見解\n",
    "\n",
    "FlashRank 的主要價值在於「輕量級」和「快速」，它為需要高效重排序的應用提供了理想的解決方案，特別適合資源受限的環境。\n",
    "\n",
    "## 學習補充重點\n",
    "\n",
    "**核心優勢：**\n",
    "- **超輕量級**：最小化記憶體和儲存需求\n",
    "- **超快速**：優化的推理速度，適合即時應用\n",
    "- **SoTA 基礎**：基於最先進的 cross-encoder 架構\n",
    "- **易於整合**：簡化的 API 設計\n",
    "\n",
    "**技術特點：**\n",
    "- **高效能設計**：專注於速度和資源使用優化\n",
    "- **即插即用**：無需複雜配置即可整合\n",
    "- **跨平台支援**：支援多種部署環境\n",
    "- **開源免費**：降低使用門檻\n",
    "\n",
    "**適用場景：**\n",
    "- **即時搜尋**：需要毫秒級響應的搜尋系統\n",
    "- **邊緣計算**：資源受限的邊緣設備\n",
    "- **高吞吐量**：大併發量的線上服務\n",
    "- **原型開發**：快速驗證重排序效果\n",
    "\n",
    "**與其他方案比較：**\n",
    "- **vs HuggingFace CrossEncoder**：更輕量、更快速\n",
    "- **vs Jina Reranker**：本地部署、無 API 依賴\n",
    "- **vs 自建方案**：開箱即用、無需訓練\n",
    "\n",
    "**部署優勢：**\n",
    "- **低資源消耗**：適合中小型應用\n",
    "- **快速部署**：減少基礎設施需求\n",
    "- **成本效益**：無額外 API 費用\n",
    "- **離線運行**：不依賴外部服務\n",
    "\n",
    "**最佳實務：**\n",
    "- 在資源敏感環境優先考慮\n",
    "- 適合需要快速回應的互動式應用\n",
    "- 可作為其他重排器的輕量級替代方案\n",
    "- 適合概念驗證和快速原型開發"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7431102d93a694f",
   "metadata": {
    "id": "c7431102d93a694f"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e9dfa010f326a",
   "metadata": {
    "id": "501e9dfa010f326a"
   },
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83ee066d91fb4f",
   "metadata": {
    "id": "7d83ee066d91fb4f"
   },
   "source": [
    "You can alternatively set OPENAI_API_KEY in ```.env``` file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set OPENAI_API_KEY in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abed94e9253ec29e",
   "metadata": {
    "id": "abed94e9253ec29e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration file to manage API keys as environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b4939",
   "metadata": {
    "id": "687b4939"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af16502c",
   "metadata": {
    "id": "af16502c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"flashrank\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43856bcf1e8f0c63",
   "metadata": {
    "id": "43856bcf1e8f0c63"
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                f\"Document {i+1}:\\n\\n{d.page_content}\\nMetadata: {d.metadata}\"\n",
    "                for i, d in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d03faa97bf809",
   "metadata": {
    "id": "1c7d03faa97bf809"
   },
   "source": [
    "## FlashrankRerank\n",
    "\n",
    "Load data for a simple example and create a **retriever**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d934121fd476be",
   "metadata": {
    "id": "79d934121fd476be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Word2Vec\n",
      "Definition: Word2Vec is a technique in NLP that maps words to a vector space, representing their semantic relationships based on context.\n",
      "Example: In a Word2Vec model, \"king\" and \"queen\" are represented by vectors located close to each other.\n",
      "Related Keywords: Natural Language Processing (NLP), Embedding, Semantic Similarity\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 12}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Embedding\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into low-dimensional continuous vectors that computers can process and understand.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing (NLP), Vectorization, Deep Learning\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 1}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "VectorStore\n",
      "Definition: A VectorStore is a system designed to store data in vector format, enabling efficient retrieval, classification, and analysis tasks.\n",
      "Example: Storing word embedding vectors in a database for quick access during semantic search.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 4}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "Definition: TF-IDF is a statistical measure used to evaluate the importance of a word within a document by considering its frequency and rarity across a corpus.\n",
      "Example: Words with high TF-IDF values are often unique and critical for understanding the document.\n",
      "Related Keywords: Natural Language Processing (NLP), Information Retrieval, Data Mining\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 18}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "GPT (Generative Pretrained Transformer)\n",
      "Definition: GPT is a generative language model pre-trained on vast datasets, capable of performing various text-based tasks. It generates natural and coherent text based on input.\n",
      "Example: A chatbot generating detailed answers to user queries is powered by GPT models.\n",
      "Related Keywords: Natural Language Processing (NLP), Text Generation, Deep Learning\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 24}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "Transformer\n",
      "Definition: A Transformer is a type of deep learning model widely used in natural language processing tasks like translation, summarization, and text generation. It is based on the Attention mechanism.\n",
      "Example: Google Translate utilizes a Transformer model for multilingual translation.\n",
      "Related Keywords: Deep Learning, Natural Language Processing (NLP), Attention mechanism\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 8}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "LLM (Large Language Model)\n",
      "Definition: LLMs are massive language models trained on large-scale text data, used for various natural language understanding and generation tasks.\n",
      "Example: OpenAI's GPT series is a prominent example of LLMs.\n",
      "Related Keywords: Natural Language Processing (NLP), Deep Learning, Text Generation\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 13}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "HuggingFace\n",
      "Definition: HuggingFace is a library offering pre-trained models and tools for natural language processing, making NLP tasks accessible to researchers and developers.\n",
      "Example: HuggingFace's Transformers library can be used for sentiment analysis and text generation.\n",
      "Related Keywords: Natural Language Processing (NLP), Deep Learning, Library.\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 9}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "Tokenizer\n",
      "Definition: A tokenizer is a tool that splits text data into tokens, often used for preprocessing in natural language processing tasks.\n",
      "Example: The sentence \"I love programming.\" is tokenized into [\"I\", \"love\", \"programming\", \".\"].\n",
      "Related Keywords: Tokenization, Natural Language Processing (NLP), Syntax Analysis.\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 3}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "Semantic Search\n",
      "Definition: Semantic search is a search technique that understands the meaning of a user's query beyond simple keyword matching, returning results that are contextually relevant.\n",
      "Example: If a user searches for \"planets in the solar system,\" the system provides information about planets like Jupiter and Mars.\n",
      "Related Keywords: Natural Language Processing (NLP), Search Algorithms, Data Mining\n",
      "Metadata: {'source': './data/appendix-keywords.txt', 'id': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Load the documents\n",
    "documents = TextLoader(\"./data/appendix-keywords.txt\").load()\n",
    "\n",
    "# Initialized the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split the documents\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Add a unique ID to each text\n",
    "for idx, text in enumerate(texts):\n",
    "    text.metadata[\"id\"] = idx\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = FAISS.from_documents(\n",
    "    texts, OpenAIEmbeddings()\n",
    ").as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# query\n",
    "query = \"Tell me about Word2Vec\"\n",
    "\n",
    "# Search for documents\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Print the document\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07e244c9171d26",
   "metadata": {
    "id": "ea07e244c9171d26"
   },
   "source": [
    "Now, let's wrap the base **retriever** with a ```ContextualCompressionRetriever``` and use ```FlashrankRerank``` as the compressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a21f9f025132c5",
   "metadata": {
    "id": "23a21f9f025132c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MultiBERT-L-12...\n",
      "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:01<00:00, 54.1MiB/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 0, 18]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 初始化 LLM（大型語言模型）\n",
    "# temperature=0 表示生成內容更穩定、可重現性高\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 初始化 FlashrankRerank 重排器\n",
    "# 這裡使用 Hugging Face 提供的 cross-encoder 模型\n",
    "# \"ms-marco-MultiBERT-L-12\" 適合多語言檢索，能根據查詢與文件的相關性重新排序\n",
    "compressor = FlashrankRerank(model=\"ms-marco-MultiBERT-L-12\")\n",
    "\n",
    "# 初始化 ContextualCompressionRetriever\n",
    "# base_compressor = 負責重排/壓縮的元件\n",
    "# base_retriever = 原本的檢索器（需事先定義 retriever）\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# 使用壓縮檢索器進行查詢\n",
    "# 它會先從 base_retriever 取得候選文件，再用 FlashrankRerank 重新排序並過濾\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Tell me about Word2Vec.\"\n",
    ")\n",
    "\n",
    "# 輸出被保留的文件 ID（假設 metadata 中有 \"id\" 欄位）\n",
    "print([doc.metadata[\"id\"] for doc in compressed_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a147fc787860bac",
   "metadata": {
    "id": "4a147fc787860bac"
   },
   "source": [
    "Compare the results after reranker is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732f27a4e8b3d4cd",
   "metadata": {
    "id": "732f27a4e8b3d4cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Word2Vec\n",
      "Definition: Word2Vec is a technique in NLP that maps words to a vector space, representing their semantic relationships based on context.\n",
      "Example: In a Word2Vec model, \"king\" and \"queen\" are represented by vectors located close to each other.\n",
      "Related Keywords: Natural Language Processing (NLP), Embedding, Semantic Similarity\n",
      "Metadata: {'id': 12, 'relevance_score': 0.9994176, 'source': './data/appendix-keywords.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Semantic Search\n",
      "Definition: Semantic search is a search technique that understands the meaning of a user's query beyond simple keyword matching, returning results that are contextually relevant.\n",
      "Example: If a user searches for \"planets in the solar system,\" the system provides information about planets like Jupiter and Mars.\n",
      "Related Keywords: Natural Language Processing (NLP), Search Algorithms, Data Mining\n",
      "Metadata: {'id': 0, 'relevance_score': 0.69165754, 'source': './data/appendix-keywords.txt'}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "Definition: TF-IDF is a statistical measure used to evaluate the importance of a word within a document by considering its frequency and rarity across a corpus.\n",
      "Example: Words with high TF-IDF values are often unique and critical for understanding the document.\n",
      "Related Keywords: Natural Language Processing (NLP), Information Retrieval, Data Mining\n",
      "Metadata: {'id': 18, 'relevance_score': 0.0027426674, 'source': './data/appendix-keywords.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Print the results of document compressions\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
