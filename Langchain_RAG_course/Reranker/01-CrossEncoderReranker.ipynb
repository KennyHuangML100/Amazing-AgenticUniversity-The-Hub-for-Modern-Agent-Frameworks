{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3l-XPuhit0_"
   },
   "source": [
    "# Cross Encoder 重排器\n",
    "\n",
    "## 概述\n",
    "Cross Encoder 重排器用於 **RAG（檢索增強生成）** 系統中，將初步檢索出的候選文件重新排序，讓最相關的內容排在前面，提升最終生成結果的精準度。\n",
    "\n",
    "---\n",
    "\n",
    "## 與 Bi-Encoder 的差別\n",
    "- **Bi-Encoder**：分別編碼查詢與文件，比對向量相似度，速度快但精度較低。\n",
    "- **Cross Encoder**：將查詢與文件組合成單一輸入，進行深度語意分析，精度高但計算較慢。\n",
    "\n",
    "---\n",
    "\n",
    "## 工作方式\n",
    "1. **向量檢索**：先用向量檢索（如 FAISS）快速抓取候選文件（例如 50 筆）。\n",
    "2. **交互編碼**：Cross Encoder 同時處理查詢與每份文件，輸出相關性分數（0–1）。\n",
    "3. **重新排序**：依分數高低重新排列，保留前 N 筆給生成模型使用。\n",
    "\n",
    "---\n",
    "\n",
    "## 優點\n",
    "- **精準度高**：理解查詢與文件的細節關係。\n",
    "- **歧義處理佳**：對多義詞、上下文有更好判斷。\n",
    "- **顯著提升相關性**：檢索效果通常可提升 10–30%。\n",
    "\n",
    "---\n",
    "\n",
    "## 缺點\n",
    "- **計算較慢**：比純向量檢索多一輪推理。\n",
    "- **資源成本高**：候選文件越多，計算時間越長。\n",
    "\n",
    "---\n",
    "\n",
    "## 最佳實務\n",
    "- **兩階段檢索**：先快後精——向量檢索 → Cross Encoder 重排。\n",
    "- **控制候選數量**：重排前 50–100 筆，最後保留 3–5 筆用於生成。\n",
    "- **批次處理**：一次送多筆文件以提升效率。\n",
    "\n",
    "---\n",
    "\n",
    "## 常用模型\n",
    "- `ms-marco-MiniLM-L-6-v2`：速度與精度平衡。\n",
    "- `ms-marco-electra-base`：精度更高，適合離線批次。\n",
    "- 多語言版本：適合跨語言檢索。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWa1F5orZZC-"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nF3eZMIEZZC-"
   },
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WU-ewAgBZZC_"
   },
   "source": [
    "You can alternatively set OPENAI_API_KEY in .env file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set OPENAI_API_KEY in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_gkL_kaZZC_"
   },
   "outputs": [],
   "source": [
    "# Configuration file to manage API keys as environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_lWOZ_eZZDA"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnVd3ga0ZZDA"
   },
   "source": [
    "## 主要功能和機制\n",
    "### 目的\n",
    "- 重新排序檢索到的文件以改善其排名，優先顯示與查詢最相關的結果。\n",
    "\n",
    "### 結構\n",
    "- 接受**查詢**和**文件**作為單一輸入對，實現聯合處理。\n",
    "\n",
    "### 機制\n",
    "- **單一輸入對**：\n",
    " 將**查詢**和**文件**作為組合輸入處理，直接輸出相關性分數。\n",
    "- **自注意力機制**：\n",
    " 使用自注意力聯合分析**查詢**和**文件**，有效捕捉其語義關係。\n",
    "\n",
    "### 優勢\n",
    "- **更高準確性**：\n",
    " 提供更精確的相似性分數。\n",
    "- **深度上下文分析**：\n",
    " 探索**查詢**和**文件**間的語義細節。\n",
    "\n",
    "### 限制\n",
    "- **高計算成本**：\n",
    " 處理過程耗時較長。\n",
    "- **擴展性問題**：\n",
    " 未經優化不適合大規模文件集合。\n",
    "\n",
    "---\n",
    "\n",
    "## 我的見解\n",
    "\n",
    "Cross Encoder 的核心價值在於語義理解的深度，它透過聯合編碼實現了比傳統向量檢索更精確的相關性評估。\n",
    "\n",
    "## 學習補充重點\n",
    "\n",
    "**關鍵技術特點：**\n",
    "- **聯合編碼**：同時處理查詢和文件，捕捉交互語義\n",
    "- **精確評分**：輸出 0-1 相關性分數\n",
    "- **上下文感知**：理解複雜的語義關係\n",
    "\n",
    "**使用時機：**\n",
    "- 檢索精度要求高的場景\n",
    "- 文件數量中等（<100 候選）\n",
    "- 可接受額外計算成本的應用\n",
    "\n",
    "**優化策略：**\n",
    "- 兩階段檢索：先粗排再精排\n",
    "- 批次處理提升效率\n",
    "- 模型量化減少計算負擔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi_JjwdrZZDB"
   },
   "source": [
    "## Practical Applications\n",
    "- A **Bi-Encoder** quickly retrieves candidate **documents** by computing lightweight similarity scores.  \n",
    "- A **Cross Encoder** refines these results by deeply analyzing the semantic relationship between the **query** and the retrieved **documents** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uccKyjmvZZDC"
   },
   "source": [
    "## Implementation\n",
    "- Use Hugging Face cross encoders or ```BAAI/bge-reranker``` models.\n",
    "- Easily integrate with frameworks like **LangChain** through the ```CrossEncoderReranker``` component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtR39M2KZZDC"
   },
   "source": [
    "## 重排器的主要優勢\n",
    "- **精確相似度評分**：\n",
    " 提供**查詢**和**文件**之間高度準確的相關性測量。\n",
    "- **語義深度**：\n",
    " 分析更深層的語義關係，揭示**查詢**-**文件**互動中的細微差別。\n",
    "- **改善搜尋品質**：\n",
    " 提升檢索到的**文件**的相關性和品質。\n",
    "- **RAG 系統增強**：\n",
    " 透過改善輸入相關性來增強**檢索增強生成（RAG）**系統的效能。\n",
    "- **無縫整合**：\n",
    " 易於適應各種工作流程，與多種框架相容。\n",
    "- **模型多樣性**：\n",
    " 提供廣泛的預訓練模型靈活性，滿足客製化需求。\n",
    "\n",
    "---\n",
    "\n",
    "## 我的見解\n",
    "\n",
    "這些優勢清楚展示了重排器在 RAG 系統中的核心價值：透過深度語義分析大幅提升檢索精度。\n",
    "\n",
    "## 學習補充重點\n",
    "\n",
    "**核心優勢應用：**\n",
    "- **精確評分**：解決向量檢索的語義偏差問題\n",
    "- **品質提升**：顯著改善最終生成內容的相關性\n",
    "- **易於部署**：與現有系統整合成本低\n",
    "- **靈活配置**：可根據不同領域選擇專門模型\n",
    "\n",
    "**實際效益：**\n",
    "- 檢索準確率提升 15-30%\n",
    "- 用戶滿意度明顯改善\n",
    "- 減少無關內容干擾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSfmqG8JZZDD"
   },
   "source": [
    "## Document Count Settings for Reranker\n",
    "- Reranking is generally performed on the top **5–10** **documents** retrieved during the initial search.\n",
    "- The ideal number of **documents** for reranking should be determined through experimentation and evaluation, as it depends on the dataset characteristics and computational resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaXF57FTZZDD"
   },
   "source": [
    "## 使用重排器的權衡考量\n",
    "- **準確性 vs 處理時間**：\n",
    " 在實現更高準確性和最小化處理時間之間取得平衡。\n",
    "- **效能改善 vs 計算成本**：\n",
    " 權衡效能改善的好處與所需額外計算資源。\n",
    "- **搜尋速度 vs 相關性準確度**：\n",
    " 管理更快檢索與維持結果高相關性之間的權衡。\n",
    "- **系統需求**：\n",
    " 確保系統滿足支援重排所需的硬體和軟體需求。\n",
    "- **資料集特性**：\n",
    " 考慮資料集的規模、多樣性和特定屬性來優化重排器效能。\n",
    "\n",
    "---\n",
    "\n",
    "## 我的見解\n",
    "\n",
    "這些權衡考量反映了實際部署重排器時需要面對的核心決策點，每個組織都需要根據自身需求找到最佳平衡點。\n",
    "\n",
    "## 學習補充重點\n",
    "\n",
    "**決策框架：**\n",
    "- **業務優先級**：效果提升 vs 成本控制\n",
    "- **用戶體驗**：響應速度 vs 結果品質\n",
    "- **資源配置**：硬體投資 vs 運營成本\n",
    "- **規模考量**：小規模精確 vs 大規模效率\n",
    "\n",
    "**實用建議：**\n",
    "- 先在小規模測試評估效益\n",
    "- 使用 A/B 測試量化改善效果\n",
    "- 考慮混合策略：關鍵查詢用重排器\n",
    "- 監控系統資源使用情況\n",
    "\n",
    "**優化策略：**\n",
    "- 非同步處理降低延遲感知\n",
    "- 快取熱門查詢結果\n",
    "- 使用輕量級模型平衡效果與速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk9Sg0AKi76R"
   },
   "source": [
    "Explaining the Implementation of **Cross Encoder Reranker** with a Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DlGOaxxozWp"
   },
   "outputs": [],
   "source": [
    "# Helper function to format and print document content\n",
    "def pretty_print_docs(docs):\n",
    "    # Print each document in the list with a separator between them\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(  # Separator line for better readability\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]  # Format: Document number + content\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obj9TBnHjFBt",
    "outputId": "f3a0ecd4-e9ba-4bcf-a509-ef38631be18a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Word2Vec\n",
      "Definition: Word2Vec is a technique in NLP that maps words to a vector space, representing their semantic relationships based on context.\n",
      "Example: In a Word2Vec model, \"king\" and \"queen\" are represented by vectors located close to each other.\n",
      "Related Keywords: Natural Language Processing (NLP), Embedding, Semantic Similarity\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Token\n",
      "Definition: A token refers to a smaller unit of text obtained by splitting a larger piece of text. It can be a word, phrase, or sentence.\n",
      "Example: The sentence \"I go to school\" can be tokenized into \"I,\" \"go,\" \"to,\" and \"school.\"\n",
      "Related Keywords: Tokenization, Natural Language Processing (NLP), Syntax Analysis\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Example: A customer information table in a relational database is an example of structured data.\n",
      "Related Keywords: Database, Data Analysis, Data Modeling\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Schema\n",
      "Definition: A schema defines the structure of a database or file, detailing how data is organized and stored.\n",
      "Example: A relational database schema specifies column names, data types, and key constraints.\n",
      "Related Keywords: Database, Data Modeling, Data Management\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "Keyword Search\n",
      "Definition: Keyword search involves finding information based on user-inputted keywords, commonly used in search engines and database systems.\n",
      "Example: Searching \n",
      "When a user searches for \"coffee shops in Seoul,\" the system returns a list of relevant coffee shops.\n",
      "Related Keywords: Search Engine, Data Search, Information Retrieval\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "Definition: TF-IDF is a statistical measure used to evaluate the importance of a word within a document by considering its frequency and rarity across a corpus.\n",
      "Example: Words with high TF-IDF values are often unique and critical for understanding the document.\n",
      "Related Keywords: Natural Language Processing (NLP), Information Retrieval, Data Mining\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "SQL\n",
      "Definition: SQL (Structured Query Language) is a programming language for managing data in databases. \n",
      "It allows you to perform various operations such as querying, updating, inserting, and deleting data.\n",
      "Example: SELECT * FROM users WHERE age > 18; retrieves information about users aged above 18.\n",
      "Related Keywords: Database, Query, Data Management\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "Open Source\n",
      "Definition: Open source software allows its source code to be freely used, modified, and distributed, fostering collaboration and innovation.\n",
      "Example: The Linux operating system is a well-known open source project.\n",
      "Related Keywords: Software Development, Community, Technical Collaboration\n",
      "Structured Data\n",
      "Definition: Structured data is organized according to a specific format or schema, making it easy to search and analyze.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      "Semantic Search\n",
      "Definition: Semantic search is a search technique that understands the meaning of a user's query beyond simple keyword matching, returning results that are contextually relevant.\n",
      "Example: If a user searches for \"planets in the solar system,\" the system provides information about planets like Jupiter and Mars.\n",
      "Related Keywords: Natural Language Processing (NLP), Search Algorithms, Data Mining\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "GPT (Generative Pretrained Transformer)\n",
      "Definition: GPT is a generative language model pre-trained on vast datasets, capable of performing various text-based tasks. It generates natural and coherent text based on input.\n",
      "Example: A chatbot generating detailed answers to user queries is powered by GPT models.\n",
      "Related Keywords: Natural Language Processing (NLP), Text Generation, Deep Learning\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load documents\n",
    "documents = TextLoader(\"./data/appendix-keywords.txt\").load()\n",
    "\n",
    "# Configure text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split documents into chunks\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# # Set up the embedding model\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/msmarco-distilbert-dot-v5\",\n",
    "    model_kwargs = {\"tokenizer_kwargs\": {\"clean_up_tokenization_spaces\": False}}\n",
    ")\n",
    "\n",
    "# Create FAISS index from documents and set up retriever\n",
    "retriever = FAISS.from_documents(texts, embeddings_model).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "# Define the query\n",
    "query = \"Can you tell me about Word2Vec?\"\n",
    "\n",
    "# Execute the query and retrieve results\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Display the retrieved documents\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_6qFjxQpUc3"
   },
   "source": [
    "現在，讓我們用 ```ContextualCompressionRetriever``` 包裝 ```base_retriever```。```CrossEncoderReranker``` 利用 ```HuggingFaceCrossEncoder``` 來重新排序檢索結果。\n",
    "\n",
    "---\n",
    "\n",
    "## 我的見解\n",
    "\n",
    "這個整合展示了如何將重排器無縫集成到現有的檢索管道中，```ContextualCompressionRetriever``` 作為包裝器提供了優雅的抽象層。\n",
    "\n",
    "## 學習補充重點\n",
    "\n",
    "**架構組成：**\n",
    "- **base_retriever**：基礎檢索器（如向量檢索）\n",
    "- **ContextualCompressionRetriever**：壓縮和優化包裝器\n",
    "- **CrossEncoderReranker**：使用 HuggingFace 模型的重排組件\n",
    "\n",
    "**技術整合優勢：**\n",
    "- **模組化設計**：可替換不同的基礎檢索器\n",
    "- **透明介面**：保持原有 API 不變\n",
    "- **效能優化**：結合檢索速度和排序精度\n",
    "\n",
    "**實作要點：**\n",
    "- 選擇合適的 CrossEncoder 模型\n",
    "- 配置重排候選數量\n",
    "- 設定相關性閾值過濾\n",
    "\n",
    "**使用場景：**\n",
    "- 需要高精度檢索的問答系統\n",
    "- 複雜查詢的文檔搜尋\n",
    "- 企業知識管理系統"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D3uj-DOpwUt"
   },
   "source": [
    "Multilingual Support BGE Reranker: [```bge-reranker-v2-m3```](https://huggingface.co/BAAI/bge-reranker-v2-m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5yrJ-FTjWJB",
    "outputId": "ddfebce9-e45a-4057-cadc-2767d7c26152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Word2Vec\n",
      "Definition: Word2Vec is a technique in NLP that maps words to a vector space, representing their semantic relationships based on context.\n",
      "Example: In a Word2Vec model, \"king\" and \"queen\" are represented by vectors located close to each other.\n",
      "Related Keywords: Natural Language Processing (NLP), Embedding, Semantic Similarity\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Open Source\n",
      "Definition: Open source software allows its source code to be freely used, modified, and distributed, fostering collaboration and innovation.\n",
      "Example: The Linux operating system is a well-known open source project.\n",
      "Related Keywords: Software Development, Community, Technical Collaboration\n",
      "Structured Data\n",
      "Definition: Structured data is organized according to a specific format or schema, making it easy to search and analyze.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "Definition: TF-IDF is a statistical measure used to evaluate the importance of a word within a document by considering its frequency and rarity across a corpus.\n",
      "Example: Words with high TF-IDF values are often unique and critical for understanding the document.\n",
      "Related Keywords: Natural Language Processing (NLP), Information Retrieval, Data Mining\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# Initialize the model\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# Select the top 3 documents\n",
    "compressor = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "# Initialize the contextual compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Retrieve compressed documents\n",
    "compressed_docs = compression_retriever.invoke(\"Can you tell me about Word2Vec?\")\n",
    "\n",
    "# Display the documents\n",
    "pretty_print_docs(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "langchain-opentutorial-W8hXPYms-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
