{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059756b7",
   "metadata": {},
   "source": [
    "# Contextual Compression Retriever（上下文壓縮檢索器）\n",
    "\n",
    "## 概覽\n",
    "\n",
    "```ContextualCompressionRetriever``` 是 LangChain 中一個強大的工具，旨在透過根據上下文壓縮所檢索的文件，來優化檢索流程。當你需要處理大量數據並根據特定查詢進行摘要或動態過濾時，此工具特別有用，能確保僅傳遞最相關的資訊至後續處理階段。\n",
    "\n",
    "### 核心特點：\n",
    "\n",
    "- **上下文感知壓縮**：根據查詢或上下文對文件進行壓縮，避免冗餘並保留關鍵資訊。\n",
    "- **靈活整合**：可與其他 LangChain 元件無縫整合，易於納入既有工作流程。\n",
    "- **壓縮方式可自訂**：可選擇不同壓縮技術（如摘要模型、向量嵌入等），滿足特定任務需求。\n",
    "\n",
    "### 適用場景：\n",
    "\n",
    "- 在問答系統中摘要大量資料。\n",
    "- 為聊天機器人提供更簡潔、相關的回覆。\n",
    "- 提升處理法律分析或學術研究等文件密集任務的效率。\n",
    "\n",
    "透過此檢索器，開發者可顯著降低計算負擔，並提升向使用者呈現資訊的品質。\n",
    "\n",
    "![](./assets/02-contextual-compression-retriever-workflow.png)  \n",
    "\n",
    "---\n",
    "\n",
    "## 目錄\n",
    "\n",
    "- [概覽](#概覽)\n",
    "- [環境設置](#環境設置)\n",
    "- [基本檢索器設定](#基本檢索器設定)\n",
    "- [上下文壓縮](#上下文壓縮)\n",
    "- [使用 LLM 進行文件過濾](#使用-llm-進行文件過濾)\n",
    "- [建立壓縮器與文件轉換器管線](#建立壓縮器與文件轉換器管線)\n",
    "\n",
    "---\n",
    "\n",
    "## 參考資料\n",
    "\n",
    "- [如何使用上下文壓縮進行檢索](https://python.langchain.com/docs/how_to/contextual_compression/)\n",
    "- [LLM ChainFilter 文件過濾器](https://python.langchain.com/api_reference/langchain/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a83b4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8e8ad1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_core\",\n",
    "        \"faiss-cpu\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Contextual Compression Retriever\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e843e",
   "metadata": {},
   "source": [
    "置換 ```OPENAI_API_KEY``` in ```.env``` 檔案在跟目錄。 \n",
    "\n",
    "[Note] This is not necessary if you've already set ```OPENAI_API_KEY``` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433c7da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da6033",
   "metadata": {},
   "source": [
    "#### 下面的函數“用來以具有視覺吸引力的格式呈現文件。”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ce609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輔助函式：以較美觀的格式輸出文件內容\n",
    "def pretty_print_docs(docs):\n",
    "    \"\"\"\n",
    "    輸入參數：\n",
    "        docs: 文件列表（通常是 LangChain 檢索結果，每個元素可能是 Document 物件，\n",
    "              其中包含 page_content 屬性存放文字內容）\n",
    "    功能：\n",
    "        將文件列表以「分隔線 + 文件標題 + 內容」的方式輸出，方便閱讀\n",
    "    \"\"\"\n",
    "    print(\n",
    "        # 使用 join 將多個文件字串用分隔線連接起來\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [\n",
    "                # 為每份文件加上「文件序號」標題，並附上文件的 page_content（內容）\n",
    "                f\"document {i+1}:\\n\\n\" + d.page_content\n",
    "                for i, d in enumerate(docs)  # enumerate 用於同時取得索引與文件\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03b400",
   "metadata": {},
   "source": [
    "## 基本檢索器設定（Basic Retriever Configuration）\n",
    "\n",
    "我們先從初始化一個簡單的向量資料庫檢索器（Vector Store Retriever）開始，並將文字文件分段儲存。\n",
    "\n",
    "當使用者提出一個問題時，這個檢索器會返回 1 至 2 筆相關文件，同時也可能包含一些不相關的文件，這反映出沒有經過上下文壓縮的檢索結果可能會混入雜訊。\n",
    "\n",
    "以下是我們建立檢索器的步驟：\n",
    "\n",
    "1. **使用 `TextLoader` 載入文字檔案**  \n",
    "   利用 LangChain 提供的 `TextLoader` 將 `.txt` 檔案中的文字載入至記憶體中。\n",
    "\n",
    "2. **使用 `CharacterTextSplitter` 將文本分段**  \n",
    "   將文字分成每段 300 個字元，且設定 `chunk_overlap=0`（不重疊），方便後續向量化處理。\n",
    "\n",
    "3. **建立向量資料庫（使用 FAISS）並轉為 Retriever**  \n",
    "   將文本段進行嵌入向量轉換並存入 FAISS 向量庫，最後使用 `.as_retriever()` 方法轉換成可查詢的檢索器。\n",
    "\n",
    "4. **向檢索器發送查詢問題**  \n",
    "   輸入一個自然語言問題，讓檢索器返回與問題最相關的內容段落。\n",
    "\n",
    "5. **列印相關段落內容**  \n",
    "   檢視檢索器返回的結果，以了解是否準確命中關鍵資訊。\n",
    "\n",
    "> 備註：這個基本設定將作為後續「上下文壓縮檢索器」的基礎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea60830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 380, which is longer than the specified 300\n",
      "Created a chunk of size 343, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 341, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 413, which is longer than the specified 300\n",
      "Created a chunk of size 310, which is longer than the specified 300\n",
      "Created a chunk of size 391, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 437, which is longer than the specified 300\n",
      "Created a chunk of size 374, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 412, which is longer than the specified 300\n",
      "Created a chunk of size 346, which is longer than the specified 300\n",
      "Created a chunk of size 403, which is longer than the specified 300\n",
      "Created a chunk of size 331, which is longer than the specified 300\n",
      "Created a chunk of size 344, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1:\n",
      "\n",
      "Multimodal\n",
      "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
      "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
      "Relate\n",
      "----------------------------------------------------------------------------------------------------\n",
      "document 2:\n",
      "\n",
      "Semantic Search\n",
      "----------------------------------------------------------------------------------------------------\n",
      "document 3:\n",
      "\n",
      "LLM (Large Language Model)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "document 4:\n",
      "\n",
      "Embedding\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader   # 用來載入純文字檔案的文件載入器\n",
    "from langchain_community.vectorstores import FAISS            # 向量資料庫 FAISS（用於相似度檢索）\n",
    "from langchain_openai import OpenAIEmbeddings                  # OpenAI 的文字嵌入模型\n",
    "from langchain_text_splitters import CharacterTextSplitter     # 文本切割工具（按字元長度分割）\n",
    "\n",
    "# 1. 使用 TextLoader 載入文字檔案\n",
    "#    這裡會將 ./data/appendix-keywords.txt 檔案讀入為一個 Document 物件\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 2. 使用 CharacterTextSplitter 將文字檔分割成小區塊\n",
    "#    chunk_size=300 表示每個文本區塊最多 300 個字元\n",
    "#    chunk_overlap=0 表示區塊之間沒有重疊\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "texts = loader.load_and_split(text_splitter)  # 載入並切割成多個文本區塊\n",
    "\n",
    "# 3. 使用 FAISS 建立向量資料庫，並將其轉為檢索器（retriever）\n",
    "#    - OpenAIEmbeddings() 會將文字轉換為向量表示\n",
    "#    - from_documents() 建立向量索引\n",
    "#    - as_retriever() 將其封裝成可以用 query 查詢的檢索器\n",
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
    "\n",
    "# 4. 使用檢索器進行查詢\n",
    "#    查詢內容是 \"What is the definition of Multimodal?\"\n",
    "#    retriever.invoke() 會回傳與查詢最相關的文件列表\n",
    "docs = retriever.invoke(\"What is the definition of Multimodal?\")\n",
    "\n",
    "# 5. 以美觀的方式輸出檢索結果\n",
    "#    這裡假設已經定義了 pretty_print_docs() 函式，用來格式化並印出每份文件內容\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358f1c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal\n",
      "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
      "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
      "Relate\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2136512",
   "metadata": {},
   "source": [
    "## 上下文壓縮（Contextual Compression）\n",
    "\n",
    "使用 ```LLMChainExtractor``` 所建立的 ```DocumentCompressor``` 正是應用於檢索器（Retriever）之上的壓縮器，也就是我們要使用的 ```ContextualCompressionRetriever```。\n",
    "\n",
    "```ContextualCompressionRetriever``` 的功能是根據查詢的上下文對檢索回來的文件進行壓縮，**刪除不相關資訊，保留最重要的內容**，以提升回答品質與效率。\n",
    "\n",
    "我們將比較應用壓縮前與壓縮後的檢索結果，來了解上下文壓縮帶來的改變。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b1fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1:\n",
      "\n",
      "Multimodal\n",
      "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
      "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
      "Relate\n",
      "----------------------------------------------------------------------------------------------------\n",
      "document 2:\n",
      "\n",
      "Semantic Search\n",
      "----------------------------------------------------------------------------------------------------\n",
      "document 3:\n",
      "\n",
      "LLM (Large Language Model)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "document 4:\n",
      "\n",
      "Embedding\n",
      "==============================================================\n",
      "===============After applying LLMChainExtractor===============\n",
      "document 1:\n",
      "\n",
      "Multimodal\n",
      "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Before applying ContextualCompressionRetriever\n",
    "pretty_print_docs(retriever.invoke(\"What is the definition of Multimodal?\"))\n",
    "print(\"=\"*62)\n",
    "print(\"=\"*15 + \"After applying LLMChainExtractor\" + \"=\"*15)\n",
    "\n",
    "\n",
    "# After applying ContextualCompressionRetriever\n",
    "# 1. Generate LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")  \n",
    "\n",
    "# 2. Generate compressor using LLMChainExtractor\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# 3. Generate compression retriever using ContextualCompressionRetriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "# 4. Query the compression retriever to find relevant documents\n",
    "compressed_docs = (\n",
    "    compression_retriever.invoke( \n",
    "        \"What is the definition of Multimodal?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Print the relevant documents\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74acacb3",
   "metadata": {},
   "source": [
    "## Document Filtering Using LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc45983",
   "metadata": {},
   "source": [
    "### LLMChainFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c71c0d",
   "metadata": {},
   "source": [
    "### 使用 LLM 進行文件篩選（LLMChainFilter）\n",
    "\n",
    "```LLMChainFilter``` 是一種簡潔但強大的壓縮器（Compressor），它透過 LLM Chain 判斷哪些文件應該被保留，哪些應該被過濾掉，適用於初步檢索後的文件集合。\n",
    "\n",
    "這個篩選器的特點是**不改變原始文件內容**，而是**選擇性地回傳符合查詢上下文的文件**，用於提升檢索的精準度與效能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4f08a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1:\n",
      "\n",
      "Multimodal\n",
      "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
      "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
      "Relate\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "# 1. Generate LLMChainFilter object using LLM\n",
    "_filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "# 2. Generate ContextualCompressionRetriever object using LLMChainFilter and retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "# 3. Query the compression retriever to find relevant documents\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What is the definition of Multimodal?\"\n",
    ")\n",
    "\n",
    "# 4. Print the relevant documents\n",
    "pretty_print_docs(compressed_docs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de64ab",
   "metadata": {},
   "source": [
    "### EmbeddingsFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246efd6",
   "metadata": {},
   "source": [
    "### 使用 EmbeddingsFilter 節省成本與提升效率\n",
    "\n",
    "對每一份檢索到的文件進行額外的 LLM 呼叫會導致成本高昂且推理速度變慢。\n",
    "\n",
    "```EmbeddingsFilter``` 提供一種更經濟且快速的替代方案，它透過將**查詢與文件進行向量嵌入（embedding）**，僅回傳與查詢**相似度足夠高的文件**。\n",
    "\n",
    "這種方法能在保持檢索結果相關性的同時，**大幅節省計算成本與時間**。\n",
    "\n",
    "整個流程如下：\n",
    "\n",
    "- 使用 ```ContextualCompressionRetriever``` 結合 ```EmbeddingsFilter``` 進行壓縮式檢索。\n",
    "- ```EmbeddingsFilter``` 將過濾掉與查詢**相似度低於設定門檻（例如 0.86）**的文件，只保留高相關性的內容供後續處理。\n",
    "\n",
    "這使得在不犧牲品質的情況下，加速了檢索流程，特別適用於資源有限或需即時回應的應用場景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106461e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1:\n",
      "\n",
      "Multimodal\n",
      "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
      "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
      "Relate\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter  # 基於嵌入向量的文件過濾器\n",
    "from langchain_openai import OpenAIEmbeddings  # OpenAI 的文字嵌入模型\n",
    "\n",
    "# 1. 建立向量嵌入模型\n",
    "#    - OpenAIEmbeddings 會將文字轉換成向量，用於計算相似度\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 2. 建立 EmbeddingsFilter 對象\n",
    "#    - embeddings: 使用上面建立的 OpenAIEmbeddings 來計算文本與查詢的相似度\n",
    "#    - similarity_threshold=0.86：設定相似度門檻（0~1之間）\n",
    "#      只有相似度 >= 0.86 的文件段落才會被保留，其餘會被過濾掉\n",
    "embeddings_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings, \n",
    "    similarity_threshold=0.86\n",
    ")\n",
    "\n",
    "# 3. 建立 ContextualCompressionRetriever\n",
    "#    - base_compressor 使用 EmbeddingsFilter（基於相似度過濾）\n",
    "#    - base_retriever 使用原本的檢索器 retriever（從向量庫中找出候選文件）\n",
    "#    作用流程：\n",
    "#      a. base_retriever 找出所有可能相關的文件\n",
    "#      b. base_compressor 計算這些文件與查詢的相似度\n",
    "#      c. 保留高於 0.86 閾值的內容\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, \n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# 4. 使用壓縮檢索器查詢\n",
    "#    - 查詢「What is the definition of Multimodal?」\n",
    "#    - 只會得到和查詢在語意上高度相關的文件段落\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What is the definition of Multimodal?\"\n",
    ")\n",
    "\n",
    "# 5. 以美觀格式輸出相關文件\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1381a0",
   "metadata": {},
   "source": [
    "## Creating a Pipeline (Compressor + Document Converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7126d",
   "metadata": {},
   "source": [
    "### 使用 `DocumentCompressorPipeline` 組合多種壓縮器與轉換器\n",
    "\n",
    "透過 ```DocumentCompressorPipeline```，可以**串接多個壓縮器（Compressor）與文件轉換器（Transformer）**，依序執行處理流程。\n",
    "\n",
    "除了壓縮器，也可以加入 ```BaseDocumentTransformer```，針對文件集合執行非語境壓縮的轉換操作。例如：\n",
    "\n",
    "- 可以使用 ```TextSplitter``` 將文件切分為較小的段落，以利後續檢索或摘要。\n",
    "- 可以使用 ```EmbeddingsRedundantFilter``` 根據嵌入相似度來移除重複文件（預設設定為相似度 ≥ 0.95 視為重複）。\n",
    "\n",
    "以下為壓縮流程範例：\n",
    "1. **文件切分**：先使用 `TextSplitter` 將原始文件分段。\n",
    "2. **移除重複**：利用 `EmbeddingsRedundantFilter` 去除語意重複的段落。\n",
    "3. **語意過濾**：依照與查詢的相似度，篩選出最相關的文件。\n",
    "\n",
    "最終結果會是一組高度相關、非重複、且已預處理的小型文件段，有助於提升檢索效率與語言模型回答品質。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91a64cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline  # 文件壓縮管線\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter   # 向量冗餘過濾器\n",
    "from langchain_text_splitters import CharacterTextSplitter                        # 文本切割工具\n",
    "\n",
    "# 1. 建立文本切割器 (CharacterTextSplitter)\n",
    "#    - chunk_size=300：每個文本塊最大 300 個字元\n",
    "#    - chunk_overlap=0：不同塊之間沒有重疊\n",
    "#    目的：將長文本拆成小塊，方便後續向量化與壓縮處理\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 2. 建立冗餘過濾器 (EmbeddingsRedundantFilter)\n",
    "#    - embeddings：使用之前建立的 OpenAIEmbeddings\n",
    "#    作用：檢測文本塊之間的語意相似度，過濾掉與其他塊幾乎重複的段落\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "\n",
    "# 3. 建立相關性過濾器 (EmbeddingsFilter)\n",
    "#    - similarity_threshold=0.86：相似度需達到 0.86 才保留\n",
    "#    作用：只保留與查詢在語意上高度相關的文本塊\n",
    "relevant_filter = EmbeddingsFilter(\n",
    "    embeddings=embeddings,\n",
    "    similarity_threshold=0.86\n",
    ")\n",
    "\n",
    "# 4. 建立文件壓縮管線 (DocumentCompressorPipeline)\n",
    "#    - transformers：依序執行的處理器列表\n",
    "#      a. splitter：切割原始文件\n",
    "#      b. redundant_filter：刪除語意重複的塊\n",
    "#      c. relevant_filter：刪除與查詢無關的塊\n",
    "#      d. LLMChainExtractor：用 LLM 從保留下來的文本中提取最重要的內容\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        splitter,\n",
    "        redundant_filter,\n",
    "        relevant_filter,\n",
    "        LLMChainExtractor.from_llm(llm),  # 使用前面建立的 llm 物件\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935e0ad",
   "metadata": {},
   "source": [
    "While initializing the  ```ContextualCompressionRetriever```, we use ```pipeline_compressor``` as the ```base_compressor``` and ```retriever``` as the ```base_retriever```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1da48dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1:\n",
      "\n",
      "Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n"
     ]
    }
   ],
   "source": [
    "# 5. Use pipeline_compressor as the base_compressor and retriever as the base_retriever to initialize ContextualCompressionRetriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "# 6. Query the compression retriever to find relevant documents\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What is the definition of Multimodal?\"\n",
    ")\n",
    "\n",
    "# 7. Print the relevant documents\n",
    "pretty_print_docs(compressed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af462c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
