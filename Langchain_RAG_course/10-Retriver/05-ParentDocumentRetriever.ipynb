{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {
    "id": "635d8ebb"
   },
   "source": [
    "# 父文件檢索器（Parent Document Retriever）\n",
    "\n",
    "## 概覽\n",
    "\n",
    "本教學聚焦於 `ParentDocumentRetriever` 的實作，一種旨在平衡「文件檢索」與「文本分塊」需求的工具。\n",
    "\n",
    "當我們將文件拆分用於搜尋時，會面臨兩種相互競爭的需求：\n",
    "\n",
    "1. **小分塊（Small Chunks）**：有助於透過嵌入向量（embeddings）準確表示語意  \n",
    "2. **上下文保留（Context Preservation）**：有助於保持文件的連貫性與完整理解\n",
    "\n",
    "> 📌 運作原理\n",
    "\n",
    "`ParentDocumentRetriever` 的運作方式如下：\n",
    "\n",
    "1. 將文件拆分成便於檢索的小分塊  \n",
    "2. 為每個小分塊保留與「原始父文件」的連結（使用 ID）  \n",
    "3. 支援透過 `TextLoader` 對多份文件進行載入與處理  \n",
    "\n",
    "> ✅ 優點\n",
    "\n",
    "1. **高效檢索**：快速找出與查詢相關的內容  \n",
    "2. **上下文感知**：可根據需求取用更完整的文件段落  \n",
    "3. **結構彈性**：可使用整份原始文件或較大的區塊作為父文件\n",
    "\n",
    "---\n",
    "\n",
    "### 目錄\n",
    "\n",
    "- [概覽](#概覽)\n",
    "- [環境設置](#環境設置)\n",
    "- [完整文件檢索](#完整文件檢索)\n",
    "- [調整較大的分塊尺寸](#調整較大的分塊尺寸)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {
    "id": "c6c7aba4"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21943adb",
   "metadata": {
    "id": "21943adb"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25ec196",
   "metadata": {
    "id": "f25ec196"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_openai\",\n",
    "        \"chromadb\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f9065ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f9065ea",
    "outputId": "419ff60c-2d09-4937-ab2d-571be3efd273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Parent-Document-Retriever\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a9ae0",
   "metadata": {
    "id": "690a9ae0"
   },
   "source": [
    "You can alternatively set API keys such as ```OPENAI_API_KEY``` in a ```.env``` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f99b5b6",
   "metadata": {
    "id": "4f99b5b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cc6f6",
   "metadata": {},
   "source": [
    "First, let's load the documents that we'll use as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6382b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    # load file (It could be multiple files)\n",
    "    TextLoader(\"./data/appendix-keywords.txt\"),\n",
    "]\n",
    "# If your os is window, execute the following line \n",
    "# loader = TextLoader(\"./data/appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    # Load the document using the loader and add it to the docs list.\n",
    "    docs.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a7d148d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='Semantic Search\\n\\nDefinition: Semantic search refers to a search method that understands the meaning behind user queries, going beyond simple keyword matching to return relevant results.  \\nExample: When a user searches for \"solar system planets,\" the search returns information about related planets like Jupiter and Mars.  \\nRelated Keywords: Natural Language Processing, Search Algorithms, Data Mining  \\n\\n\\nEmbedding\\n\\nDefinition: Embedding is the process of converting textual data, such as words or sentences, into low-dimensional continuous vectors. This enables computers to understand and process text.  \\nExample: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].  \\nRelated Keywords: Natural Language Processing, Vectorization, Deep Learning  \\n\\n\\nToken\\n\\nDefinition: A token refers to smaller units of text obtained by breaking it into parts, such as words, sentences, or phrases.  \\nExample: The sentence \"I go to school\" can be split into tokens like \"I,\" \"go,\" \"to,\" and \"school.\"  \\nRelated Keywords: Tokenization, Natural Language Processing, Syntax Analysis  \\n\\n\\nTokenizer\\n\\nDefinition: A tokenizer is a tool used to divide text data into tokens, often as part of preprocessing in natural language processing.  \\nExample: The sentence \"I love programming.\" is split into [\"I\", \"love\", \"programming\", \".\"].  \\nRelated Keywords: Tokenization, Natural Language Processing, Syntax Analysis  \\n\\n\\nVectorStore\\n\\nDefinition: A vector store is a system that stores data transformed into vector formats. It is used in tasks like search, classification, and data analysis.  \\nExample: Word embedding vectors are stored in a database for quick access.  \\nRelated Keywords: Embedding, Database, Vectorization  \\n\\n\\nSQL\\n\\nDefinition: SQL (Structured Query Language) is a programming language used to manage data in databases. It supports tasks like querying, modifying, inserting, and deleting data.  \\nExample: `SELECT * FROM users WHERE age > 18;` retrieves information about users over 18 years old.  \\nRelated Keywords: Database, Query, Data Management  \\n\\n\\nCSV\\n\\nDefinition: CSV (Comma-Separated Values) is a file format used to store data, where each value is separated by a comma. It is often used for saving and exchanging tabular data.  \\nExample: A CSV file with headers \"Name, Age, Job\" could contain data like \"John, 30, Developer.\"  \\nRelated Keywords: Data Format, File Handling, Data Exchange  \\n\\n\\nJSON\\n\\nDefinition: JSON (JavaScript Object Notation) is a lightweight data interchange format that uses text to represent data objects in a readable manner for both humans and machines.  \\nExample: `{\"Name\": \"John\", \"Age\": 30, \"Job\": \"Developer\"}` is an example of JSON-formatted data.  \\nRelated Keywords: Data Exchange, Web Development, API  \\n\\n\\nTransformer\\n\\nDefinition: A transformer is a type of deep learning model used in natural language processing for tasks like translation, summarization, and text generation. It is based on the attention mechanism.  \\nExample: Google Translate uses transformer models for multilingual translations.  \\nRelated Keywords: Deep Learning, Natural Language Processing, Attention  \\n\\n\\nHuggingFace\\n\\nDefinition: HuggingFace is a library that provides pre-trained models and tools for natural language processing, making it easier for researchers and developers to perform NLP tasks.  \\nExample: Using HuggingFace\\'s Transformers library to perform sentiment analysis or text generation.  \\nRelated Keywords: Natural Language Processing, Deep Learning, Library  \\n\\n\\nDigital Transformation\\n\\nDefinition: Digital transformation refers to the process of leveraging technology to innovate services, culture, and operations within a business, improving business models and competitiveness through digital technologies.  \\nExample: A company adopting cloud computing to revolutionize data storage and processing.  \\nRelated Keywords: Innovation, Technology, Business Model  \\n\\n\\nCrawling\\n\\nDefinition: Crawling is the automated process of visiting web pages to collect data. It is commonly used for search engine optimization and data analysis.  \\nExample: Google’s search engine crawls websites to collect and index content.  \\nRelated Keywords: Data Collection, Web Scraping, Search Engine  \\n\\n\\nWord2Vec\\n\\nDefinition: Word2Vec is a natural language processing technique that maps words to vector spaces, representing semantic relationships between words.  \\nExample: In a Word2Vec model, \"king\" and \"queen\" are represented as vectors close to each other in the vector space.  \\nRelated Keywords: Natural Language Processing, Embedding, Semantic Similarity  \\n\\n\\nLLM (Large Language Model)\\n\\nDefinition: An LLM is a large-scale language model trained on massive text datasets, used for various natural language understanding and generation tasks.  \\nExample: OpenAI\\'s GPT series is a prominent example of LLMs.  \\nRelated Keywords: Natural Language Processing, Deep Learning, Text Generation  \\n\\n\\nFAISS (Facebook AI Similarity Search)\\n\\nDefinition: FAISS is a high-speed similarity search library developed by Facebook, designed for efficiently searching large sets of vectors for similar ones.  \\nExample: Using FAISS to quickly find similar images in a dataset containing millions of image vectors.  \\nRelated Keywords: Vector Search, Machine Learning, Database Optimization  \\n\\n\\nOpen Source\\n\\nDefinition: Open source refers to software where the source code is publicly available for anyone to use, modify, and distribute, promoting collaboration and innovation.  \\nExample: The Linux operating system is a widely known open-source project.  \\nRelated Keywords: Software Development, Community, Technology Collaboration  \\n\\n\\nStructured Data\\nDefinition: Organized data following a predefined format or schema, easily searchable and analyzable in databases and spreadsheets.\\nExample: Customer information table stored in a relational database.\\nRelated Keywords: database, data analysis, data modeling\\n\\n\\nParser\\nDefinition: A tool that analyzes given data (strings, files, etc.) and converts it into a structured format, used in programming language syntax analysis and file data processing.\\nExample: Parsing HTML documents to create DOM structure of web pages.\\nRelated Keywords: syntax analysis, compiler, data processing\\n\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)\\nDefinition: A statistical measure used to evaluate word importance in documents, considering both word frequency in a document and its rarity across all documents.\\nExample: Words that appear infrequently across many documents have high TF-IDF values.\\nRelated Keywords: natural language processing, information retrieval, data mining\\n\\n\\nDeep Learning\\nDefinition: A machine learning field using artificial neural networks to solve complex problems, focusing on learning high-level data representations.\\nExample: Used in image recognition, speech recognition, and natural language processing.\\nRelated Keywords: artificial neural networks, machine learning, data analysis\\n\\n\\nSchema\\nDefinition: Blueprint defining database or file structure, specifying how data is stored and organized.\\nExample: Relational database table schemas define column names, data types, and key constraints.\\nRelated Keywords: database, data modeling, data management\\n\\n\\nDataFrame\\nDefinition: Table-like data structure with rows and columns, primarily used for data analysis and processing.\\nExample: Pandas library DataFrames can contain various data types and facilitate data manipulation and analysis.\\nRelated Keywords: data analysis, pandas, data processing\\n\\n\\nAttention Mechanism\\nDefinition: A deep learning technique that enables models to focus more on important information, primarily used with sequence data.\\nExample: Translation models use attention to focus on important parts of input sentences for accurate translation.\\nRelated Keywords: deep learning, natural language processing, sequence modeling\\n\\n\\nPandas\\nDefinition: A Python library providing tools for data analysis and manipulation, enabling efficient data analysis tasks.\\nExample: Used to read CSV files, clean data, and perform various analyses.\\nRelated Keywords: data analysis, Python, data processing\\n\\n\\nGPT (Generative Pretrained Transformer)\\nDefinition: A pre-trained generative language model for various text-based tasks, capable of generating natural language based on input text.\\nExample: Chatbots generating detailed responses to user questions using GPT models.\\nRelated Keywords: natural language processing, text generation, deep learning\\n\\n\\nInstructGPT\\nDefinition: An optimized GPT model designed to perform specific tasks based on user instructions, producing more accurate and relevant results.\\nExample: Creating email drafts based on user instructions.\\nRelated Keywords: artificial intelligence, natural language understanding, instruction-based processing\\n\\n\\nKeyword Search\\nDefinition: Process of finding information based on user-input keywords, fundamental to most search engines and database systems.\\nExample: Searching \"coffee shop Seoul\" returns relevant coffee shop listings.\\nRelated Keywords: search engine, data search, information retrieval\\n\\n\\nPage Rank\\nDefinition: Algorithm evaluating webpage importance by analyzing web page link structures, used in search engine result ranking.\\nExample: Google search engine uses PageRank to determine search result rankings.\\nRelated Keywords: search engine optimization, web analytics, link analysis\\n\\n\\nData Mining\\nDefinition: Process of discovering useful information from large datasets using statistics, machine learning, and pattern recognition.\\nExample: Retail stores analyzing customer purchase data to develop sales strategies.\\nRelated Keywords: big data, pattern recognition, predictive analytics\\n\\n\\nMultimodal\\nDefinition: Technology processing multiple data modes (text, images, sound, etc.) to extract or predict more accurate information through interaction between different data formats.\\nExample: Systems analyzing both images and descriptive text for more accurate image classification.\\nRelated Keywords: data fusion, artificial intelligence, deep learning')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15356c17",
   "metadata": {},
   "source": [
    "## 完整文件檢索（Full Document Retrieval）\n",
    "\n",
    "在此模式中，我們的目標是對**完整文件**進行搜尋。因此，我們只需指定 `child_splitter`（子分割器）。\n",
    "\n",
    "稍後，我們也會加入 `parent_splitter`（父分割器）來比較檢索結果的差異。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db5c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Child Splitter with chunk size\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "\n",
    "# Create a Chroma DB collection -- in memory version\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Create Retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4faa20",
   "metadata": {},
   "source": [
    "Documents are added using the ```retriever.add_documents(docs, ids=None)``` function:\n",
    "* If ```ids``` is ```None```, they will be automatically generated.\n",
    "* Setting ```add_to_docstore=False``` prevents duplicate document additions. However, ```ids``` values are required to check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab710019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to the retriever. 'docs' is a list of documents, and 'ids' is a list of unique document identifiers.\n",
    "retriever.add_documents(docs, ids=None, add_to_docstore=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46704a",
   "metadata": {},
   "source": [
    "This code should return two keys because we added two documents.\n",
    "\n",
    "- Convert the keys returned by the ```store``` object's ```yield_keys()``` method into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e34a4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['739c2480-1aac-4090-ae21-ceebc416d099']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return all keys from the store as a list.\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c82732",
   "metadata": {},
   "source": [
    "讓我們嘗試呼叫向量資料庫（vector store）的搜尋功能。\n",
    "\n",
    "由於我們儲存的是**較小的區塊（chunks）**，因此在搜尋結果中應該會看到這些小區塊被返回。\n",
    "\n",
    "你可以使用向量資料庫物件的 `similarity_search` 方法來執行相似度搜尋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d84a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec\n"
     ]
    }
   ],
   "source": [
    "# Perform similarity search\n",
    "sub_docs = vectorstore.similarity_search(\"Word2Vec\")\n",
    "\n",
    "# Print the page_content property of the first element in the sub_docs list.\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe1682",
   "metadata": {},
   "source": [
    "Now let's search through the entire retriever. In this process, since it **returns the documents** containing the small chunks, relatively larger documents will be returned.\n",
    "\n",
    "Use the ```invoke()``` method of the ```retriever``` object to retrieve documents related to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c9d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and fetch documents\n",
    "retrieved_docs = retriever.invoke(\"Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c9be3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 10044\n",
      "\n",
      "=====================\n",
      "\n",
      " old.  \n",
      "Related Keywords: Database, Query, Data Management  \n",
      "\n",
      "\n",
      "CSV\n",
      "\n",
      "Definition: CSV (Comma-Separated Values) is a file format used to store data, where each value is separated by a comma. It is often used for saving and exchanging tabular data.  \n",
      "Example: A CSV file with headers \"Name, Age, Job\" could contain data like \"John, 30, Developer.\"  \n",
      "Related Keywords: Data Format, File Handling, Data Exchange  \n",
      "\n",
      "\n",
      "JSON\n",
      "\n",
      "Definition: JSON (JavaScript Object Notation) is a lightweight data interchange form\n"
     ]
    }
   ],
   "source": [
    "# Print the length of the page content of the retrieved document\n",
    "print(\n",
    "    f\"Document length: {len(retrieved_docs[0].page_content)}\",\n",
    "    end=\"\\n\\n=====================\\n\\n\",\n",
    ")\n",
    "\n",
    "# Print a portion of the document\n",
    "print(retrieved_docs[0].page_content[2000:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25659b35",
   "metadata": {},
   "source": [
    "## 調整較大的區塊大小（Adjusting Larger Chunk Sizes）\n",
    "\n",
    "如同先前的結果，**整份文件可能過大，不適合直接用於搜尋**。\n",
    "\n",
    "在這種情況下，我們實際上希望的作法是：\n",
    "\n",
    "1. 先將原始文件切分成**較大的區塊（Parent Chunks）**。\n",
    "2. 再將這些較大的區塊細分為**更小的區塊（Child Chunks）**。\n",
    "\n",
    "接著，我們會：\n",
    "- 對**小區塊進行向量索引（index）**\n",
    "- 在檢索時返回**較大的父區塊**（而不是整份文件）\n",
    "\n",
    "### 實作方式：\n",
    "\n",
    "使用 `RecursiveCharacterTextSplitter` 建立父與子的切分器：\n",
    "\n",
    "- 父文件（Parent Documents）的 `chunk_size` 設為 `1000`\n",
    "- 子文件（Child Documents）的 `chunk_size` 設為 `200`，使其比父文件小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a398e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text splitter used to generate parent documents\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "\n",
    "# Text splitter used to generate child documents\n",
    "# Should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "\n",
    "# Vector store to be used for indexing child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "# Storage layer for parent documents\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e752ae5",
   "metadata": {},
   "source": [
    "這是初始化 ```ParentDocumentRetriever``` 的程式碼說明：\n",
    "\n",
    "- `vectorstore` 參數：指定用於儲存文件向量的向量資料庫。\n",
    "- `docstore` 參數：指定用於儲存文件實體內容的文件儲存庫。\n",
    "- `child_splitter` 參數：指定用來切分子文件的分割器。\n",
    "- `parent_splitter` 參數：指定用來切分父文件的分割器。\n",
    "\n",
    "```ParentDocumentRetriever``` 能夠處理具有階層結構的文件，會分別切分與儲存父文件與子文件。\n",
    "\n",
    "這樣的設計讓我們在檢索時，能夠有效結合父文件的上下文與子文件的精確內容，提升搜尋品質與語境理解能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "660df0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # Specify the vector store\n",
    "    vectorstore=vectorstore,\n",
    "    # Specify the document store\n",
    "    docstore=store,\n",
    "    # Specify the child document splitter\n",
    "    child_splitter=child_splitter,\n",
    "    # Specify the parent document splitter\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aec94e",
   "metadata": {},
   "source": [
    "Add docs to the ```retriever``` object. This adds new documents to the set of documents that ```retriever``` can search through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1068426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to the retriever\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882ce91",
   "metadata": {},
   "source": [
    "Now you can see there are many more documents. These are the larger chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24f79410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate keys from the store, convert to list, and return the length\n",
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "469eb179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec\n"
     ]
    }
   ],
   "source": [
    "# Perform similarity search\n",
    "sub_docs = vectorstore.similarity_search(\"Word2Vec\")\n",
    "# Print the page_content property of the first element in the sub_docs list\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a62a1",
   "metadata": {},
   "source": [
    "Now let's use the ```invoke()``` method of the ```retriever``` object to search for documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcddd245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling\n",
      "\n",
      "Definition: Crawling is the automated process of visiting web pages to collect data. It is commonly used for search engine optimization and data analysis.  \n",
      "Example: Google’s search engine crawls websites to collect and index content.  \n",
      "Related Keywords: Data Collection, Web Scraping, Search Engine  \n",
      "\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a natural language processing technique that maps words to vector spaces, representing semantic relationships between words.  \n",
      "Example: In a Word2Vec model, \"king\" and \"queen\" are represented as vectors close to each other in the vector space.  \n",
      "Related Keywords: Natural Language Processing, Embedding, Semantic Similarity  \n",
      "\n",
      "\n",
      "LLM (Large Language Model)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and fetch documents\n",
    "retrieved_docs = retriever.invoke(\"Word2Vec\")\n",
    "\n",
    "# Return the length of the page content of the first retrieved document\n",
    "print(retrieved_docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
