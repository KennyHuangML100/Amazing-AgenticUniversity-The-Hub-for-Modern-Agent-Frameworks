{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4a0d60",
   "metadata": {},
   "source": [
    "# 向量存儲支持的檢索器\n",
    "\n",
    "## 概覽\n",
    "本教程提供了使用 LangChain 建立和優化**向量存儲支持的檢索器**的全面指南。它涵蓋了使用 FAISS（Facebook AI 相似性搜索）創建向量存儲的基礎步驟，並探索了提高搜索準確性和效率的高級檢索策略。\n",
    "\n",
    "**向量存儲支持的檢索器**是一個文檔檢索系統，它利用向量存儲根據文檔的向量表示來搜索文檔。這種方法能夠高效地進行基於相似性的搜索，用於處理非結構化數據。\n",
    "\n",
    "### RAG（檢索增強生成）工作流程\n",
    "![rag-flow.png](./assets/01-vectorstore-retriever-rag-flow.png)\n",
    "\n",
    "上圖說明了 RAG 系統中的**文檔搜索和回應生成**工作流程。\n",
    "\n",
    "步驟包括：\n",
    "\n",
    "1. 文檔載入：導入原始文檔。\n",
    "2. 文本分塊：將文本分割成可管理的塊。\n",
    "3. 向量嵌入：使用嵌入模型將文本轉換為數值向量。\n",
    "4. 存儲在向量資料庫：將生成的嵌入存儲在向量資料庫中以實現高效檢索。\n",
    "\n",
    "在查詢階段：\n",
    "- 步驟：用戶查詢 → 嵌入 → 在向量存儲中搜索 → 檢索相關塊 → LLM 生成回應\n",
    "- 用戶查詢使用嵌入模型轉換為嵌入向量。\n",
    "- 這個查詢嵌入與向量資料庫中存儲的文檔向量進行比較，以**檢索最相關的結果**。\n",
    "- 檢索到的塊被傳遞給大型語言模型（LLM），該模型基於檢索到的信息生成最終回應。\n",
    "\n",
    "本教程旨在探索和優化「向量存儲 → 檢索相關塊 → LLM 生成回應」階段。它將涵蓋高級檢索技術以提高回應的準確性和相關性。\n",
    "\n",
    "## 目錄\n",
    "\n",
    "- [概覽](#概覽)\n",
    "- [環境設定](#環境設定)\n",
    "- [初始化和使用 VectorStoreRetriever](#初始化和使用-vectorstoreretriever)\n",
    "- [動態配置（使用 ConfigurableField）](#動態配置使用-configurablefield)\n",
    "- [使用分離的查詢和段落嵌入模型](#使用分離的查詢和段落嵌入模型)\n",
    "\n",
    "## 參考資料\n",
    "\n",
    "- [如何使用向量存儲作為檢索器](https://python.langchain.com/docs/how_to/vectorstore_retriever/)\n",
    "- [最大邊際相關性（MMR）](https://community.fullstackretrieval.com/retrieval-methods/maximum-marginal-relevance)\n",
    "- [Upstage-Embeddings](https://console.upstage.ai/docs/capabilities/embeddings)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6af75",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions, and utilities for tutorials. \n",
    "- You can checkout out the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a9e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c34330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain_opentutorial\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_upstage\",\n",
    "        \"faiss-cpu\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bf9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        # \"OPENAI_API_KEY\": \"\",\n",
    "        # \"LANGCHAIN_API_KEY\": \"\",\n",
    "        # \"UPSTAGE_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"VectorStore Retriever\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c24f41",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as ```OPENAI_API_KEY``` in a ```.env``` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfaffe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration file to manage the API KEY as an environment variable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API KEY information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e66b00",
   "metadata": {},
   "source": [
    "## Initializing and Using VectorStoreRetriever\n",
    "\n",
    "This section demonstrates how to load documents using OpenAI embeddings and create a vector database using FAISS.\n",
    "\n",
    "- The example below showcases how to use OpenAI embeddings for document loading and FAISS for vector database creation.\n",
    "- Once the vector database is created, it can be loaded and queried using retrieval methods such as **Similarity Search** and **Maximal Marginal Relevance (MMR)** to search for relevant text within the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc586a1",
   "metadata": {},
   "source": [
    "📌 **Creating a Vector Store (Using FAISS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9b98ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 343, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 341, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the file using TextLoader\n",
    "loader = TextLoader(\"./data/01-vectorstore-retriever-appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(documents) # Split into smaller chunks\n",
    "\n",
    "# Initialize the OpenAI embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a FAISS vector database\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbd962",
   "metadata": {},
   "source": [
    "📌 **1. Initializing and Using VectorStoreRetriever (```as_retriever``` )**\n",
    "\n",
    "The ```as_retriever``` method allows you to convert a vector database into a retriever, enabling efficient document search and retrieval from the vector store.\n",
    "\n",
    "**How It Works**:\n",
    "* The ```as_retriever()``` method transforms a vector store (like FAISS) into a retriever object, making it compatible with LangChain's retrieval workflows.\n",
    "* This retriever can then be directly used with RAG pipelines or combined with Large Language Models (LLMs) for building intelligent search systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d21318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Retriever Creation (Similarity Search)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51d7bc",
   "metadata": {},
   "source": [
    "**高級檢索器配置**\n",
    "\n",
    "```as_retriever``` 方法允許您配置高級檢索策略，例如**相似性搜索**、**MMR（最大邊際相關性）**和**基於相似度分數閾值的過濾**。\n",
    "\n",
    "**參數：**\n",
    "\n",
    "- ```**kwargs```：傳遞給檢索函數的關鍵字參數：\n",
    "   - ```search_type```：指定搜索方法。\n",
    "     - ```\"similarity\"```：基於餘弦相似度返回最相關的文檔。\n",
    "     - ```\"mmr\"```：使用最大邊際相關性算法，平衡**相關性**和**多樣性**。\n",
    "     - ```\"similarity_score_threshold\"```：返回相似度分數高於指定閾值的文檔。\n",
    "   - ```search_kwargs```：用於微調結果的額外搜索選項：\n",
    "     - ```k```：要返回的文檔數量（預設：```4```）。\n",
    "     - ```score_threshold```：```\"similarity_score_threshold\"``` 搜索類型的最低相似度分數（例如：```0.8```）。\n",
    "     - ```fetch_k```：MMR 搜索期間初始檢索的文檔數量（預設：```20```）。\n",
    "     - ```lambda_mult```：控制 MMR 結果中的多樣性（```0``` = 最大多樣性，```1``` = 最大相關性，預設：```0.5```）。\n",
    "     - ```filter```：用於選擇性文檔檢索的元數據過濾。\n",
    "\n",
    "**返回值：**\n",
    "\n",
    "- ```VectorStoreRetriever```：可直接用於文檔搜索任務查詢的初始化檢索器對象。\n",
    "\n",
    "**注意事項：**\n",
    "- 支援多種搜索策略（```similarity```、```MMR```、```similarity_score_threshold```）。\n",
    "- MMR 通過減少結果中的冗餘來改善結果多樣性，同時保持相關性。\n",
    "- 元數據過濾能夠基於文檔屬性進行選擇性文檔檢索。\n",
    "- ```tags``` 參數可用於標記檢索器，以便更好地組織和更容易識別。\n",
    "\n",
    "**注意事項：**\n",
    "- MMR 的多樣性控制：\n",
    "  - 仔細調整 ```fetch_k```（初始檢索的文檔數量）和 ```lambda_mult```（多樣性控制因子）以達到最佳平衡。\n",
    "  - ```lambda_mult```\n",
    "    - 較低值（< 0.5）→ 優先考慮多樣性。\n",
    "    - 較高值（> 0.5）→ 優先考慮相關性。\n",
    "  - 將 ```fetch_k``` 設定得比 ```k``` 高，以實現有效的多樣性控制。\n",
    "- 閾值設定：\n",
    "  - 使用過高的 ```score_threshold```（例如 0.95）可能導致零結果。\n",
    "- 元數據過濾：\n",
    "  - 在應用過濾器之前確保元數據結構定義良好。\n",
    "- 平衡配置：\n",
    "  - 在 ```search_type``` 和 ```search_kwargs``` 設定之間保持適當平衡，以獲得最佳檢索性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c038da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector format, often used for search, classification, and data analysis tasks.\n",
      "Example: Storing word embeddings in a database for fast retrieval of similar words.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "Definition: Semantic search is a method of retrieving results based on the meaning of the user's query, going beyond simple keyword matching.\n",
      "Example: If a user searches for \"solar system planets,\" the search returns information about related planets like Jupiter and Mars.\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "Definition: Keyword search is the process of finding information based on specific keywords entered by the user. It is commonly used in search engines and database systems as a fundamental search method.\n",
      "Example: If a user searches for \"coffee shop in Seoul,\" the search engine returns a list of related coffee shops.\n",
      "Related Keywords: Search Engine, Data Retrieval, Information Search\n",
      "Definition: FAISS is a high-speed similarity search library developed by Facebook, designed for efficient vector searches in large datasets.\n",
      "Example: Searching for similar images in a dataset of millions using FAISS.\n",
      "Related Keywords: Vector Search, Machine Learning, Database Optimization\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\n",
    "        \"k\": 5,  # Return the top 5 most relevant documents\n",
    "        \"score_threshold\": 0.7  # Only return documents with a similarity score of 0.7 or higher\n",
    "    }\n",
    ")\n",
    "# Perform the search\n",
    "query = \"Explain the concept of vector search.\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Display search results\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d741849",
   "metadata": {},
   "source": [
    "### 檢索器的 ```invoke()``` 方法\n",
    "\n",
    "```invoke()``` 方法是與檢索器互動的主要入口點。它用於基於給定查詢搜索和檢索相關文檔。\n",
    "\n",
    "**運作方式**：\n",
    "1. 查詢提交：用戶查詢作為輸入提供。\n",
    "2. 嵌入生成：查詢被轉換為向量表示（如有必要）。\n",
    "3. 搜索過程：檢索器使用指定的搜索策略（相似性、MMR 等）搜索向量資料庫。\n",
    "4. 結果返回：該方法返回相關文檔塊的列表。\n",
    "\n",
    "**參數：**\n",
    "- ```input```（必需）：\n",
    "   - 用戶提供的查詢字串。\n",
    "   - 查詢被轉換為向量，並與存儲的文檔向量進行比較，以進行基於相似性的檢索。\n",
    "\n",
    "- ```config```（可選）：\n",
    "   - 允許對檢索過程進行細粒度控制。\n",
    "   - 可用於指定**標籤、元數據插入和搜索策略**。\n",
    "\n",
    "- ```**kwargs```（可選）：\n",
    "   - 能夠直接傳遞 ```search_kwargs``` 進行高級配置。\n",
    "   - 範例選項包括：\n",
    "     - ```k```：要返回的文檔數量。\n",
    "     - ```score_threshold```：文檔被包含的最低相似度分數。\n",
    "     - ```fetch_k```：MMR 搜索中初始檢索的文檔數量。\n",
    "\n",
    "**返回值：**\n",
    "- ```List[Document]```：\n",
    "   - 返回包含檢索文本和元數據的文檔對象列表。\n",
    "   - 每個文檔對象包括：\n",
    "     - ```page_content```：文檔的主要內容。\n",
    "     - ```metadata```：與文檔相關的元數據（例如：來源、標籤）。\n",
    "\n",
    "**使用說明：**\n",
    "\n",
    "### invoke() 方法的核心功能\n",
    "\n",
    "**1. 簡潔的檢索接口**\n",
    "```python\n",
    "# 基本使用\n",
    "results = retriever.invoke(\"查詢文本\")\n",
    "\n",
    "# 帶配置的使用\n",
    "results = retriever.invoke(\n",
    "    \"查詢文本\",\n",
    "    config={\"tags\": [\"search\"]},\n",
    "    k=5,\n",
    "    score_threshold=0.7\n",
    ")\n",
    "```\n",
    "\n",
    "**2. 靈活的參數控制**\n",
    "- **即時調整**：可在不重新初始化檢索器的情況下調整搜索參數\n",
    "- **動態配置**：根據查詢類型動態調整檢索策略\n",
    "- **元數據利用**：充分利用文檔元數據進行精確檢索\n",
    "\n",
    "**3. 統一的回傳格式**\n",
    "- 所有檢索器都返回相同的 `Document` 格式\n",
    "- 便於後續處理和鏈式操作\n",
    "- 保持與 LangChain 生態系統的一致性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51834bb6",
   "metadata": {},
   "source": [
    "**Usage Example 1: Basic Usage (Synchronous Search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257d2ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "=========================================================\n",
      "Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "=========================================================\n",
      "Semantic Search\n",
      "=========================================================\n",
      "Deep Learning\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is an embedding?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d5ba5",
   "metadata": {},
   "source": [
    "**Usage Example 2: Search with Options** ( ```search_kwargs``` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643a6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector format, often used for search, classification, and data analysis tasks.\n",
      "Example: Storing word embeddings in a database for fast retrieval of similar words.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# search options: top 5 results with a similarity score ≥ 0.7\n",
    "docs = retriever.invoke(\n",
    "    \"What is a vector database?\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.7}\n",
    ")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e360a59",
   "metadata": {},
   "source": [
    "**Usage Example 3: Using** ```config``` **and** ```**kwargs``` **(Advanced Configuration)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58154d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 [Search Result 1]\n",
      "📄 Document Content: Definition: A DataFrame is a tabular data structure with rows and columns, commonly used for data analysis and manipulation.\n",
      "Example: Pandas DataFrame can store data like an Excel sheet and perform operations like filtering and grouping.\n",
      "Related Keywords: Data Analysis, Pandas, Data Manipulation\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "🔍 [Search Result 2]\n",
      "📄 Document Content: Schema\n",
      "\n",
      "Definition: A schema defines the structure of a database or file, describing how data is stored and organized.\n",
      "Example: A database schema can specify table columns, data types, and constraints.\n",
      "Related Keywords: Database, Data Modeling, Data Management\n",
      "\n",
      "DataFrame\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "🔍 [Search Result 3]\n",
      "📄 Document Content: Pandas\n",
      "\n",
      "Definition: Pandas is a Python library for data analysis and manipulation, offering tools for working with structured data.\n",
      "Example: Pandas can read CSV files, clean data, and perform statistical analysis.\n",
      "Related Keywords: Data Analysis, Python, Data Manipulation\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "🔍 [Search Result 4]\n",
      "📄 Document Content: Data Mining\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Create a RunnableConfig with tags and metadata\n",
    "config = RunnableConfig(\n",
    "    tags=[\"retrieval\", \"faq\"],  ## Adding tags for query categorization\n",
    "    metadata={\"project\": \"vectorstore-tutorial\"}  # Project-specific metadata for traceability\n",
    ")\n",
    "# Perform a query using advanced configuration settings\n",
    "docs = retriever.invoke(\n",
    "    input=\"What is a DataFrame?\", \n",
    "    config=config,  # Applying the config with tags and metadata\n",
    "    search_kwargs={\n",
    "        \"k\": 3,                   \n",
    "        \"score_threshold\": 0.8   \n",
    "    }\n",
    ")\n",
    "#  Display the search results\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"\\n🔍 [Search Result {idx + 1}]\")\n",
    "    print(\"📄 Document Content:\", doc.page_content)\n",
    "    print(\"🗂️ Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab94c2c",
   "metadata": {},
   "source": [
    "## 最大邊際相關性 (MMR)\n",
    "\n",
    "**最大邊際相關性 (MMR)** 搜尋方法是一種文件檢索演算法，透過平衡相關性和多樣性來減少冗餘，提供更好的搜尋結果。\n",
    "\n",
    "**MMR 運作原理：**\n",
    "與僅基於相似度分數返回最相關文件的基本相似性搜尋不同，MMR 考慮兩個關鍵因素：\n",
    "1. 相關性：衡量文件與使用者查詢的匹配程度。\n",
    "2. 多樣性：確保檢索到的文件彼此不同，避免重複性結果。\n",
    "\n",
    "**關鍵參數：**\n",
    "- ```search_type=\"mmr\"```：啟用 MMR 檢索策略。\n",
    "- ```k```：應用多樣性過濾後返回的文件數量（預設：```4```）。\n",
    "- ```fetch_k```：應用多樣性過濾前初始檢索的文件數量（預設：```20```）。\n",
    "- ```lambda_mult```：多樣性控制因子（```0 = 最大多樣性```，```1 = 最大相關性```，預設：```0.5```）。\n",
    "\n",
    "---\n",
    "\n",
    "## 我的見解\n",
    "\n",
    "MMR 是資訊檢索中非常實用的演算法，特別適用於需要避免結果重複的場景。它解決了傳統相似性搜尋的一個重要問題：當使用者搜尋某個主題時，往往會得到許多內容相似的文件，這降低了搜尋的實用性。\n",
    "\n",
    "MMR 的核心價值在於其平衡機制 - 透過 lambda_mult 參數，使用者可以根據需求調整相關性與多樣性的權重。\n",
    "\n",
    "## 學習補充重點\n",
    "\n",
    "**實際應用場景：**\n",
    "- 推薦系統：避免推薦相似商品\n",
    "- 搜尋引擎：提供多元化的搜尋結果\n",
    "- 文件摘要：選擇代表性段落\n",
    "\n",
    "**參數調整策略：**\n",
    "- 探索性搜尋時：降低 lambda_mult（增加多樣性）\n",
    "- 精確查找時：提高 lambda_mult（增加相關性）\n",
    "- fetch_k 通常設為 k 的 3-5 倍效果較佳\n",
    "\n",
    "**注意事項：**\n",
    "- MMR 計算複雜度較高，可能影響回應速度\n",
    "- 需要適當的向量化模型支援才能有效運作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8144a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 [Query]: What is an embedding?\n",
      "\n",
      "📄 [Document 1]\n",
      "📖 Document Content: Embedding\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "📄 [Document 2]\n",
      "📖 Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "📄 [Document 3]\n",
      "📖 Document Content: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# MMR Retriever Configuration (Balancing Relevance and Diversity)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,                \n",
    "        \"fetch_k\": 10,           \n",
    "        \"lambda_mult\": 0.6  # Balancing Similarity and Diversity (0.6: Slight Emphasis on Diversity)\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"What is an embedding?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "#  Display the search results\n",
    "print(f\"\\n🔎 [Query]: {query}\\n\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"📄 [Document {idx + 1}]\")\n",
    "    print(\"📖 Document Content:\", doc.page_content)\n",
    "    print(\"🗂️ Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902babfe",
   "metadata": {},
   "source": [
    "## 相似度分數閾值搜尋\n",
    "\n",
    "**相似度分數閾值搜尋** 是一種檢索方法，只返回超過預定義相似度分數的文件。這種方法有助於過濾低相關性結果，確保返回的文件與查詢高度相關。\n",
    "\n",
    "**主要特點：**\n",
    "- 相關性過濾：只返回相似度分數高於指定閾值的文件。\n",
    "- 可配置精確度：可使用 ```score_threshold``` 參數調整閾值。\n",
    "- 搜尋類型啟用：透過設定 ```search_type=\"similarity_score_threshold\"``` 來啟用。\n",
    "\n",
    "這種搜尋方法非常適合需要**高度精確**結果的任務，例如事實查核或回答技術性問題。\n",
    "\n",
    "---\n",
    "\n",
    "## 我的見解\n",
    "\n",
    "相較於 MMR 著重平衡性，閾值搜尋更注重「質量控制」。它採用「寧缺勿濫」的策略，確保每個返回的結果都達到最低品質標準。這在需要高準確性的應用場景中特別有價值。\n",
    "\n",
    "這種方法的優勢在於可預測性 - 使用者可以明確知道所有結果都符合設定的相關性標準，避免了傳統 top-k 搜尋可能返回低品質結果的問題。\n",
    "\n",
    "## 學習補充重點\n",
    "\n",
    "**適用場景：**\n",
    "- 醫療診斷輔助：需要高度準確的資訊\n",
    "- 法律文件檢索：精確性至關重要\n",
    "- 技術文檔查詢：避免誤導性資訊\n",
    "\n",
    "**閾值設定策略：**\n",
    "- 高閾值（0.8-0.9）：追求極高精確度，可能犧牲召回率\n",
    "- 中等閾值（0.6-0.8）：平衡精確度與召回率\n",
    "- 低閾值（0.4-0.6）：確保基本相關性，提高召回率\n",
    "\n",
    "**與其他方法比較：**\n",
    "- vs Top-k：可能返回不同數量的結果\n",
    "- vs MMR：不考慮多樣性，純粹基於相關性\n",
    "- 可與 MMR 結合：先用閾值過濾，再用 MMR 增加多樣性\n",
    "\n",
    "**實作考量：**\n",
    "- 需要根據向量模型特性調整閾值\n",
    "- 建議先進行小規模測試確定最佳閾值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6509f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 [Query]: What is Word2Vec?\n",
      "\n",
      "📄 [Document 1]\n",
      "📖 Document Content: Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "📄 [Document 2]\n",
      "📖 Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "📄 [Document 3]\n",
      "📖 Document Content: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "📄 [Document 4]\n",
      "📖 Document Content: Tokenizer\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "📄 [Document 5]\n",
      "📖 Document Content: Semantic Search\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Retriever Configuration (Similarity Score Threshold Search)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",  \n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.6,  \n",
    "        \"k\": 5                \n",
    "    }\n",
    ")\n",
    "# Execute the query\n",
    "query = \"What is Word2Vec?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Display the search results \n",
    "print(f\"\\n🔎 [Query]: {query}\\n\")\n",
    "if docs:\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(f\"📄 [Document {idx + 1}]\")\n",
    "        print(\"📖 Document Content:\", doc.page_content)\n",
    "        print(\"🗂️ Metadata:\", doc.metadata)\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"⚠️ No relevant documents found. Try lowering the similarity score threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16c14f",
   "metadata": {},
   "source": [
    "### Configuring ```top_k``` (Adjusting the Number of Returned Documents)\n",
    "\n",
    "- The parameter ```k``` specifies the number of documents returned during a vector search. It determines how many of the **top-ranked** documents (based on similarity score) will be retrieved from the vector database.\n",
    "\n",
    "- The number of documents retrieved can be adjusted by setting the ```k``` value within the ```search_kwargs```.  \n",
    "- For example, setting ```k=1``` will return only the **top 1 most relevant document** based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081e0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 [Query]: What is an embedding?\n",
      "\n",
      "📄 [Document 1]\n",
      "📖 Document Content: Embedding\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Retriever Configuration (Return Only the Top 1 Document)\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 1  # Return only the top 1 most relevant document\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"What is an embedding?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "#  Display the search results \n",
    "print(f\"\\n🔎 [Query]: {query}\\n\")\n",
    "if docs:\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(f\"📄 [Document {idx + 1}]\")\n",
    "        print(\"📖 Document Content:\", doc.page_content)\n",
    "        print(\"🗂️ Metadata:\", doc.metadata)\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"⚠️ No relevant documents found. Try increasing the `k` value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168ca53",
   "metadata": {},
   "source": [
    "## Dynamic Configuration (Using ```ConfigurableField``` )\n",
    "\n",
    "The ```ConfigurableField``` feature in LangChain allows for **dynamic adjustment** of search configurations, providing flexibility during query execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Runtime Search Configuration: Adjust search settings without modifying the core retriever setup.\n",
    "- Enhanced Traceability: Assign unique identifiers, names, and descriptions to each parameter for improved readability and debugging.\n",
    "- Flexible Control with ```config```: Search configurations can be passed dynamically using the ```config``` parameter as a dictionary.\n",
    "\n",
    "\n",
    "**Use Cases:**\n",
    "- Switching Search Strategies: Dynamically adjust the search type (e.g., ```\"similarity\"```, ```\"mmr\"``` ).\n",
    "- Real-Time Parameter Adjustments: Modify search parameters like ```k``` , ```score_threshold``` , and ```fetch_k``` during query execution.\n",
    "- Experimentation: Easily test different search strategies and parameter combinations without rewriting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc25029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField \n",
    "\n",
    "# Retriever Configuration Using ConfigurableField\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1}).configurable_fields(\n",
    "    search_type=ConfigurableField(\n",
    "        id=\"search_type\", \n",
    "        name=\"Search Type\",  # Name for the search strategy\n",
    "        description=\"The search type to use\",  # Description of the search strategy\n",
    "    ),\n",
    "    search_kwargs=ConfigurableField(\n",
    "        id=\"search_kwargs\",  \n",
    "        name=\"Search Kwargs\",  # Name for the search parameters\n",
    "        description=\"The search kwargs to use\",  # Description of the search parameters\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b750c",
   "metadata": {},
   "source": [
    "The following examples demonstrate how to apply dynamic search settings using ```ConfigurableField``` in LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f53c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 [Search Results - Basic Configuration (Top 3 Documents)]\n",
      "📄 [Document 1]\n",
      "Embedding\n",
      "============================================================\n",
      "📄 [Document 2]\n",
      "Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "============================================================\n",
      "📄 [Document 3]\n",
      "Semantic Search\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ✅ Search Configuration 1: Basic Search (Top 3 Documents)\n",
    "\n",
    "config_1 = {\"configurable\": {\"search_kwargs\": {\"k\": 3}}}\n",
    "\n",
    "# Execute the query\n",
    "docs = retriever.invoke(\"What is an embedding?\", config=config_1)\n",
    "\n",
    "# Display the search results\n",
    "print(\"\\n🔎 [Search Results - Basic Configuration (Top 3 Documents)]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"📄 [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951851c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 [Search Results - Similarity Score Threshold ≥ 0.8]\n",
      "📄 [Document 1]\n",
      "Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ✅ Search Configuration 2: Similarity Score Threshold (≥ 0.8)\n",
    "\n",
    "config_2 = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"similarity_score_threshold\",\n",
    "        \"search_kwargs\": {\n",
    "            \"score_threshold\": 0.8,  # Only return documents with a similarity score of 0.8 or higher\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the query\n",
    "docs = retriever.invoke(\"What is Word2Vec?\", config=config_2)\n",
    "\n",
    "# Display the search results\n",
    "print(\"\\n🔎 [Search Results - Similarity Score Threshold ≥ 0.8]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"📄 [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e6e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 [Search Results - MMR (Diversity and Relevance Balanced)]\n",
      "📄 [Document 1]\n",
      "Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "============================================================\n",
      "📄 [Document 2]\n",
      "Tokenizer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ✅ Search Configuration 3: MMR Search (Diversity and Relevance Balanced)\n",
    "\n",
    "config_3 = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"search_kwargs\": {\n",
    "            \"k\": 2,            # Return the top 2 most diverse and relevant documents\n",
    "            \"fetch_k\": 10,     # Initially fetch the top 10 documents before filtering for diversity\n",
    "            \"lambda_mult\": 0.6 # Balance factor: 0.6 (0 = maximum diversity, 1 = maximum relevance)\n",
    "        },\n",
    "    }\n",
    "}\n",
    "# Execute the query using MMR search\n",
    "docs = retriever.invoke(\"What is Word2Vec?\", config=config_3)\n",
    "\n",
    "#  Display the search results\n",
    "print(\"\\n🔎 [Search Results - MMR (Diversity and Relevance Balanced)]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"📄 [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddf405",
   "metadata": {},
   "source": [
    "## Using Separate Query & Passage Embedding Models\n",
    "\n",
    "By default, a retriever uses the **same embedding model** for both queries and documents. However, certain scenarios can benefit from using different models tailored to the specific needs of queries and documents.\n",
    "\n",
    "### Why Use Separate Embedding Models?\n",
    "Using different models for queries and documents can improve retrieval accuracy and search relevance by optimizing each model for its intended purpose:\n",
    "- Query Embedding Model: Fine-tuned for understanding short and concise search queries.\n",
    "- Document (Passage) Embedding Model: Optimized for longer text spans with richer context.\n",
    "  \n",
    "For instance, **Upstage Embeddings** provides the capability to use distinct models for:  \n",
    "- Query Embeddings (```solar-embedding-1-large-query```)  \n",
    "- Document (Passage) Embeddings (```solar-embedding-1-large-passage```)  \n",
    "\n",
    "In such cases, the query is embedded using the query embedding model, while the documents are embedded using the document embedding model. \n",
    "\n",
    "✅ **How to Issue an Upstage API Key**  \n",
    "- Sign Up & Log In: \n",
    "   - Visit [Upstage](https://upstage.ai/) and log in (sign up if you don't have an account).  \n",
    "\n",
    "- Open API Key Page:\n",
    "   - Go to the menu bar, select \"Dashboards\", then navigate to \"API Keys\".\n",
    "\n",
    "- Generate API Key:  \n",
    "   - Click **\"Create new key\"** → Enter name your key (e.g., ```LangChain-Tutorial```) \n",
    "\n",
    "- Copy & Store Safely:  \n",
    "   - Copy the generated key and keep it secure.  \n",
    "\n",
    "<img src=\"./assets/01-vectorstore-retriever-get-upstage-api-key.png\" alt=\"Description\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b51093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 343, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 341, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "# ✅ 1. Data Loading and Document Splitting\n",
    "loader = TextLoader(\"./data/01-vectorstore-retriever-appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the loaded documents into text chunks \n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# ✅ 2. Document Embedding\n",
    "doc_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "\n",
    "# ✅ 3. Create a Vector Database\n",
    "db = FAISS.from_documents(split_docs, doc_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f69e7",
   "metadata": {},
   "source": [
    "The following example demonstrates the process of generating an Upstage embedding for a query, converting the query sentence into a vector, and conducting a vector similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa46e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 [Query]: What is an embedding?\n",
      "\n",
      "📄 [Document 1]\n",
      "📖 Document Content: Embedding\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "📄 [Document 2]\n",
      "📖 Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "🗂️ Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ✅ 3. Query Embedding and Vector Search\n",
    "query_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n",
    "\n",
    "# Convert the query into a vector using the query embedding model\n",
    "query_vector = query_embedder.embed_query(\"What is an embedding?\")\n",
    "\n",
    "# ✅ 4. Vector Similarity Search (Return Top 2 Documents)\n",
    "results = db.similarity_search_by_vector(query_vector, k=2)\n",
    "\n",
    "# ✅ 5. Display the Search Results\n",
    "print(f\"\\n🔎 [Query]: What is an embedding?\\n\")\n",
    "for idx, doc in enumerate(results):\n",
    "    print(f\"📄 [Document {idx + 1}]\")\n",
    "    print(\"📖 Document Content:\", doc.page_content)\n",
    "    print(\"🗂️ Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-bMU5IxA3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
