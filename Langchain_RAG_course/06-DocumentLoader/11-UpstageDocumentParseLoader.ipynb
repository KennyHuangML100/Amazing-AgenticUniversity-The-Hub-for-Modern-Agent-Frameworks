{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概覽\n",
    "\n",
    "```UpstageDocumentParseLoader``` 是 Upstage 所設計的一款強大文件分析工具，可作為 LangChain 框架中的文件載入器使用。其專長在於透過分析文件版面與內容，將其轉換為結構化的 HTML 格式。\n",
    "\n",
    "**主要特色**：\n",
    "\n",
    "- **完整的版面分析**：  \n",
    "  能分析並辨識多種文件格式（如 PDF、圖片）中的結構元素，例如標題、段落、表格與圖片。\n",
    "\n",
    "- **自動化結構辨識**：  \n",
    "  根據閱讀順序自動偵測並序列化文件中的元素，準確轉換為 HTML 結構。\n",
    "\n",
    "- **可選的 OCR 支援**：  \n",
    "  提供光學文字辨識（OCR）功能，處理掃描或圖片型文件。OCR 模式包含：\n",
    "  \n",
    "  - ```force```：強制使用 OCR 從圖片中擷取文字。\n",
    "  - ```auto```：自動從 PDF 中擷取文字（若輸入非 PDF 將拋出錯誤）。\n",
    "\n",
    "透過辨識並保留文件元素間的邏輯關係，```UpstageDocumentParseLoader``` 能實現精準且具上下文意識的文件分析。\n",
    "\n",
    "**從 Layout Analysis 遷移說明**：  \n",
    "Upstage 推出了 Document Parse，以取代原有的 Layout Analysis！新版本支援更多文件格式，並具備 markdown 輸出、圖表偵測、數學公式辨識等新功能，更多特性也在持續開發中。舊版 Layout Analysis（layout-analysis-0.4.0）將於 2024 年 11 月 10 日正式終止維護。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents \n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Key Changes from Layout Analysis](#key-changes-from-layout-analysis)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [UpstageDocumentParseLoader Key Parameters](#upstagedocumentparseloader-key-parameters)\n",
    "- [Usage Example](#usage-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 與 Layout Analysis 的主要差異（Key Changes from Layout Analysis）\n",
    "\n",
    "**現有選項的變更**：\n",
    "\n",
    "1. ```use_ocr``` → ```ocr```  \n",
    "   原本的 ```use_ocr``` 選項已被 ```ocr``` 取代。  \n",
    "   原先使用布林值（```True/False```），現在改為接受更具控制性的字串值：  \n",
    "   - ```force```：強制進行 OCR  \n",
    "   - ```auto```：自動辨識 PDF 並執行 OCR（非 PDF 將報錯）\n",
    "\n",
    "2. ```output_type``` → ```output_format```  \n",
    "   原先用於指定輸出格式的 ```output_type``` 已重新命名為 ```output_format```，語意更清晰。\n",
    "\n",
    "3. ```exclude``` → ```base64_encoding```  \n",
    "   用來排除特定元素的 ```exclude``` 選項，已被 ```base64_encoding``` 所取代。  \n",
    "   現在可以指定是否將某些類別的元素（如圖像）以 Base64 編碼格式保留輸出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [UpstageDocumentParseLoader](https://python.langchain.com/api_reference/upstage/document_parse/langchain_upstage.document_parse.UpstageDocumentParseLoader.html)\n",
    "- [UpstageLayoutAnalysisLoader](https://python.langchain.com/api_reference/upstage/layout_analysis/langchain_upstage.layout_analysis.UpstageLayoutAnalysisLoader.html)\n",
    "- [Upstage Migrate to Document Parse from Layout Analysis](https://console.upstage.ai/docs/capabilities/document-parse/migration-dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]** \n",
    "\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n",
    "\n",
    "\n",
    "### API Key Configuration\n",
    "To use ```UpstageDocumentParseLoader``` , you need to [obtain a Upstage API key](https://console.upstage.ai/api-keys).\n",
    "\n",
    "Once you have your API key, set it as the value for the variable ```UPSTAGE_API_KEY``` .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain_upstage\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"UPSTAGE_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"12-UpstageDocumentParseLoader\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set ```UPSTAGE_API_KEY``` in .env file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set ```UPSTAGE_API_KEY``` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow async\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpstageDocumentParseLoader 主要參數說明\n",
    "\n",
    "- ```file_path```：  \n",
    "  欲分析的文件路徑，可為單一檔案或多個檔案。\n",
    "\n",
    "- ```split```：  \n",
    "  文件切分模式  \n",
    "  可選值：`'none'`（不切分）、`'element'`（依元素切分）、`'page'`（依頁面切分）  \n",
    "  預設值：`'none'`\n",
    "\n",
    "- ```model```：  \n",
    "  用於文件解析的模型名稱  \n",
    "  預設值：`'document-parse'`\n",
    "\n",
    "- ```ocr```：  \n",
    "  OCR 模式  \n",
    "  可選值：  \n",
    "  - `\"force\"`：一律使用 OCR 進行文字擷取  \n",
    "  - `\"auto\"`：僅在輸入為 PDF 時啟用 OCR\n",
    "\n",
    "- ```output_format```：  \n",
    "  分析結果的輸出格式  \n",
    "  可選值：`'html'`、`'text'`、`'markdown'`  \n",
    "  預設值：`'html'`\n",
    "\n",
    "- ```coordinates```：  \n",
    "  是否在輸出中包含 OCR 區塊的座標位置  \n",
    "  預設值：`True`\n",
    "\n",
    "- ```base64_encoding```：  \n",
    "  指定哪些元素類別需以 Base64 編碼輸出  \n",
    "  可接受的類別包括：  \n",
    "  `'paragraph'`、`'table'`、`'figure'`、`'header'`、`'footer'`、`'list'`、`'chart'` 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用範例（Usage Example）\n",
    "\n",
    "現在讓我們透過一個實際範例，來執行 ```UpstageDocumentParseLoader```。\n",
    "\n",
    "### 資料準備\n",
    "\n",
    "在本教學中，我們將使用以下 PDF 檔案進行文件分析：\n",
    "\n",
    "- 下載連結：  \n",
    "  [Modular-RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks](https://arxiv.org/abs/2407.21059)\n",
    "\n",
    "- 檔案名稱：`2407.21059.pdf`  \n",
    "- 檔案路徑：`./data/2407.21059.pdf`\n",
    "\n",
    "請從上述連結下載該 PDF 文件，並在你目前的工作目錄下建立一個名為 `data` 的資料夾，將檔案儲存至該資料夾中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded and saved to: ./data/2407.21059.pdf\n"
     ]
    }
   ],
   "source": [
    "# Download and save sample PDF file to ./data directory\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_pdf(url, save_path):\n",
    "    \"\"\"\n",
    "    Downloads a PDF file from the given URL and saves it to the specified path.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the PDF file to download.\n",
    "        save_path (str): The full path (including file name) where the file will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Save the file to the specified path\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "\n",
    "        print(f\"PDF downloaded and saved to: {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while downloading the file: {e}\")\n",
    "\n",
    "\n",
    "# Configuration for the PDF file\n",
    "pdf_url = \"https://arxiv.org/pdf/2407.21059\"\n",
    "file_path = \"./data/2407.21059.pdf\"\n",
    "\n",
    "# Download the PDF\n",
    "download_pdf(pdf_url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file path\n",
    "FILE_PATH = \"data/2407.21059.pdf\"  # modify to your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='<p id='0' data-category='paragraph' style='font-size:14px'>1</p> <h1 id='1' style='font-size:20px'>Modular RAG: Transforming RAG Systems into<br>LEGO-like Reconfigurable Frameworks</h1> <br><p id='2' data-category='paragraph' style='font-size:18px'>Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang</p> <p id='3' data-category='paragraph' style='font-size:16px'>Abstract—Retrieval-augmented Generation (RAG) has<br>markedly enhanced the capabilities of Large Language Models<br>(LLMs) in tackling knowledge-intensive tasks. The increasing<br>demands of application scenarios have driven the evolution<br>of RAG, leading to the integration of advanced retrievers,<br>LLMs and other complementary technologies, which in turn<br>has amplified the intricacy of RAG systems. However, the rapid<br>advancements are outpacing the foundational RAG paradigm,<br>with many methods struggling to be unified under the process<br>of “retrieve-then-generate”. In this context, this paper examines<br>the limitations of the existing RAG paradigm and introduces<br>the modular RAG framework. By decomposing complex RAG<br>systems into independent modules and specialized operators, it<br>facilitates a highly reconfigurable framework. Modular RAG<br>transcends the traditional linear architecture, embracing a<br>more advanced design that integrates routing, scheduling, and<br>fusion mechanisms. Drawing on extensive research, this paper<br>further identifies prevalent RAG patterns—linear, conditional,<br>branching, and looping—and offers a comprehensive analysis<br>of their respective implementation nuances. Modular RAG<br>presents innovative opportunities for the conceptualization<br>and deployment of RAG systems. Finally, the paper explores<br>the potential emergence of new operators and paradigms,<br>establishing a solid theoretical foundation and a practical<br>roadmap for the continued evolution and practical deployment<br>of RAG technologies.</p> <br><p id='4' data-category='paragraph' style='font-size:16px'>Index Terms—Retrieval-augmented generation, large language<br>model, modular system, information retrieval</p> <p id='5' data-category='paragraph' style='font-size:18px'>I. INTRODUCTION</p> <br><header id='6' style='font-size:22px'>2024<br>Jul<br>26<br>[cs.CL]<br>arXiv:2407.21059v1</header> <br><p id='7' data-category='paragraph' style='font-size:18px'>L remarkable capabilities, yet they still face numerous<br>ARGE Language Models (LLMs) have demonstrated<br>challenges, such as hallucination and the lag in information up-<br>dates [1]. Retrieval-augmented Generation (RAG), by access-<br>ing external knowledge bases, provides LLMs with important<br>contextual information, significantly enhancing their perfor-<br>mance on knowledge-intensive tasks [2]. Currently, RAG, as<br>an enhancement method, has been widely applied in various<br>practical application scenarios, including knowledge question<br>answering, recommendation systems, customer service, and<br>personal assistants. [3]–[6]</p> <br><p id='8' data-category='paragraph' style='font-size:18px'>During the nascent stages of RAG , its core framework is<br>constituted by indexing, retrieval, and generation, a paradigm<br>referred to as Naive RAG [7]. However, as the complexity<br>of tasks and the demands of applications have escalated, the</p> <p id='9' data-category='footnote' style='font-size:14px'>Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous<br>Systems, Tongji University, Shanghai, 201210, China.<br>Yun Xiong is with Shanghai Key Laboratory of Data Science, School of<br>Computer Science, Fudan University, Shanghai, 200438, China.<br>Meng Wang and Haofen Wang are with College of Design and Innovation,<br>Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen<br>Wang. E-mail: carter.whfcarter@gmail.com)</p> <br><p id='10' data-category='paragraph' style='font-size:18px'>limitations of Naive RAG have become increasingly apparent.<br>As depicted in Figure 1, it predominantly hinges on the<br>straightforward similarity of chunks, result in poor perfor-<br>mance when confronted with complex queries and chunks with<br>substantial variability. The primary challenges of Naive RAG<br>include: 1) Shallow Understanding of Queries. The semantic<br>similarity between a query and document chunk is not always<br>highly consistent. Relying solely on similarity calculations<br>for retrieval lacks an in-depth exploration of the relationship<br>between the query and the document [8]. 2) Retrieval Re-<br>dundancy and Noise. Feeding all retrieved chunks directly<br>into LLMs is not always beneficial. Research indicates that<br>an excess of redundant and noisy information may interfere<br>with the LLM’s identification of key information, thereby<br>increasing the risk of generating erroneous and hallucinated<br>responses. [9]</p> <br><p id='11' data-category='paragraph' style='font-size:18px'>To overcome the aforementioned limitations, Advanced<br>RAG paradigm focuses on optimizing the retrieval phase,<br>aiming to enhance retrieval efficiency and strengthen the<br>utilization of retrieved chunks. As shown in Figure 1 ,typical<br>strategies involve pre-retrieval processing and post-retrieval<br>processing. For instance, query rewriting is used to make<br>the queries more clear and specific, thereby increasing the<br>accuracy of retrieval [10], and the reranking of retrieval results<br>is employed to enhance the LLM’s ability to identify and<br>utilize key information [11].</p> <br><p id='12' data-category='paragraph' style='font-size:18px'>Despite the improvements in the practicality of Advanced<br>RAG, there remains a gap between its capabilities and real-<br>world application requirements. On one hand, as RAG tech-<br>nology advances, user expectations rise, demands continue to<br>evolve, and application settings become more complex. For<br>instance, the integration of heterogeneous data and the new<br>demands for system transparency, control, and maintainability.<br>On the other hand, the growth in application demands has<br>further propelled the evolution of RAG technology.</p> <br><p id='13' data-category='paragraph' style='font-size:18px'>As shown in Figure 2, to achieve more accurate and efficient<br>task execution, modern RAG systems are progressively inte-<br>grating more sophisticated function, such as organizing more<br>refined index base in the form of knowledge graphs, integrat-<br>ing structured data through query construction methods, and<br>employing fine-tuning techniques to enable encoders to better<br>adapt to domain-specific documents.</p> <br><p id='14' data-category='paragraph' style='font-size:18px'>In terms of process design, the current RAG system has<br>surpassed the traditional linear retrieval-generation paradigm.<br>Researchers use iterative retrieval [12] to obtain richer con-<br>text, recursive retrieval [13] to handle complex queries, and<br>adaptive retrieval [14] to provide overall autonomy and flex-<br>ibility. This flexibility in the process significantly enhances</p>' metadata={'page': 1, 'base64_encodings': [], 'coordinates': [[{'x': 0.9137, 'y': 0.0321}, {'x': 0.9206, 'y': 0.0321}, {'x': 0.9206, 'y': 0.0418}, {'x': 0.9137, 'y': 0.0418}], [{'x': 0.1037, 'y': 0.0715}, {'x': 0.8961, 'y': 0.0715}, {'x': 0.8961, 'y': 0.1385}, {'x': 0.1037, 'y': 0.1385}], [{'x': 0.301, 'y': 0.149}, {'x': 0.6988, 'y': 0.149}, {'x': 0.6988, 'y': 0.1673}, {'x': 0.301, 'y': 0.1673}], [{'x': 0.0785, 'y': 0.2203}, {'x': 0.4943, 'y': 0.2203}, {'x': 0.4943, 'y': 0.5498}, {'x': 0.0785, 'y': 0.5498}], [{'x': 0.0785, 'y': 0.5566}, {'x': 0.4926, 'y': 0.5566}, {'x': 0.4926, 'y': 0.5837}, {'x': 0.0785, 'y': 0.5837}], [{'x': 0.2176, 'y': 0.6044}, {'x': 0.3518, 'y': 0.6044}, {'x': 0.3518, 'y': 0.6205}, {'x': 0.2176, 'y': 0.6205}], [{'x': 0.0254, 'y': 0.2747}, {'x': 0.0612, 'y': 0.2747}, {'x': 0.0612, 'y': 0.7086}, {'x': 0.0254, 'y': 0.7086}], [{'x': 0.0764, 'y': 0.625}, {'x': 0.4947, 'y': 0.625}, {'x': 0.4947, 'y': 0.7904}, {'x': 0.0764, 'y': 0.7904}], [{'x': 0.0774, 'y': 0.7923}, {'x': 0.4942, 'y': 0.7923}, {'x': 0.4942, 'y': 0.8539}, {'x': 0.0774, 'y': 0.8539}], [{'x': 0.0773, 'y': 0.8701}, {'x': 0.4946, 'y': 0.8701}, {'x': 0.4946, 'y': 0.9447}, {'x': 0.0773, 'y': 0.9447}], [{'x': 0.5068, 'y': 0.221}, {'x': 0.9234, 'y': 0.221}, {'x': 0.9234, 'y': 0.4605}, {'x': 0.5068, 'y': 0.4605}], [{'x': 0.5074, 'y': 0.4636}, {'x': 0.9243, 'y': 0.4636}, {'x': 0.9243, 'y': 0.6131}, {'x': 0.5074, 'y': 0.6131}], [{'x': 0.5067, 'y': 0.6145}, {'x': 0.9234, 'y': 0.6145}, {'x': 0.9234, 'y': 0.7483}, {'x': 0.5067, 'y': 0.7483}], [{'x': 0.5071, 'y': 0.7504}, {'x': 0.9236, 'y': 0.7504}, {'x': 0.9236, 'y': 0.8538}, {'x': 0.5071, 'y': 0.8538}], [{'x': 0.5073, 'y': 0.8553}, {'x': 0.9247, 'y': 0.8553}, {'x': 0.9247, 'y': 0.9466}, {'x': 0.5073, 'y': 0.9466}]]}\n",
      "page_content='<header id='15' style='font-size:14px'>2</header> <figure id='16'><img style='font-size:16px' alt=\"Fig. 1. Cases of Naive RAG and Advanced RAG.When faced with complex\n",
      "questions, both encounter limitations and struggle to provide satisfactory\n",
      "answers. Despite the fact that Advanced RAG improves retrieval accuracy\n",
      "through hierarchical indexing, pre-retrieval, and post-retrieval processes, these\n",
      "relevant documents have not been used correctly.\" data-coord=\"top-left:(99,107); bottom-right:(1168,1179)\" /></figure> <br><p id='17' data-category='paragraph' style='font-size:20px'>the expressive power and adaptability of RAG systems, en-<br>abling them to better adapt to various application scenarios.<br>However, this also makes the orchestration and scheduling of<br>workflows more complex, posing greater challenges to system<br>design. Specifically, RAG currently faces the following new<br>challenges:</p> <br><p id='18' data-category='paragraph' style='font-size:20px'>Complex data sources integration. RAG are no longer<br>confined to a single type of unstructured text data source but<br>have expanded to include various data types, such as semi-<br>structured data like tables and structured data like knowledge<br>graphs [15]. Access to heterogeneous data from multiple<br>sources can provide the system with a richer knowledge<br>background, and more reliable knowledge verification capa-<br>bilities [16].</p> <br><caption id='19' style='font-size:16px'>Fig. 2. Case of current Modular RAG.The system integrates diverse data<br>and more functional components. The process is no longer confined to linear<br>but is controlled by multiple control components for retrieval and generation,<br>making the entire system more flexible and complex.</caption> <br><p id='20' data-category='paragraph' style='font-size:20px'>New demands for system interpretability, controllability,</p>' metadata={'page': 2, 'base64_encodings': [], 'coordinates': [[{'x': 0.9113, 'y': 0.0313}, {'x': 0.9227, 'y': 0.0313}, {'x': 0.9227, 'y': 0.0417}, {'x': 0.9113, 'y': 0.0417}], [{'x': 0.0777, 'y': 0.0649}, {'x': 0.9163, 'y': 0.0649}, {'x': 0.9163, 'y': 0.7149}, {'x': 0.0777, 'y': 0.7149}], [{'x': 0.0775, 'y': 0.7174}, {'x': 0.4936, 'y': 0.7174}, {'x': 0.4936, 'y': 0.8062}, {'x': 0.0775, 'y': 0.8062}], [{'x': 0.0762, 'y': 0.8089}, {'x': 0.4923, 'y': 0.8089}, {'x': 0.4923, 'y': 0.9293}, {'x': 0.0762, 'y': 0.9293}], [{'x': 0.5077, 'y': 0.8791}, {'x': 0.9248, 'y': 0.8791}, {'x': 0.9248, 'y': 0.926}, {'x': 0.5077, 'y': 0.926}], [{'x': 0.0911, 'y': 0.9311}, {'x': 0.4948, 'y': 0.9311}, {'x': 0.4948, 'y': 0.9462}, {'x': 0.0911, 'y': 0.9462}]]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import UpstageDocumentParseLoader\n",
    "\n",
    "# Configure the document loader\n",
    "loader = UpstageDocumentParseLoader(\n",
    "    FILE_PATH,\n",
    "    output_format=\"html\",\n",
    "    split=\"page\",\n",
    "    ocr=\"auto\",\n",
    "    coordinates=True,\n",
    "    base64_encoding=[\"chart\"],\n",
    ")\n",
    "\n",
    "# Load the document\n",
    "docs = loader.load()\n",
    "\n",
    "# Print the results\n",
    "for doc in docs[:2]:\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-F0L5SJfm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
