{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字元分割器（Character Text Splitter）\n",
    "\n",
    "## 概述\n",
    "\n",
    "在使用 LangChain 進行文件處理時，文字分割是一個非常關鍵的步驟。\n",
    "\n",
    "```CharacterTextSplitter``` 提供了高效的文字分塊功能，其主要優點包括：\n",
    "\n",
    "- **Token 限制處理：** 解決大型語言模型（LLM）上下文視窗長度的限制問題\n",
    "- **搜尋最佳化：** 支援更精準的文字區塊檢索\n",
    "- **記憶體效率：** 有效處理大型文件，減少記憶體使用量\n",
    "- **上下文保留：** 透過 ```chunk_overlap``` 保持文字區塊之間的語義連貫性\n",
    "\n",
    "本教學將介紹文字分割的實際應用，包括核心方法 ```split_text()``` 和 ```create_documents()``` 的使用方式，並探討如何處理進階功能如 metadata（中繼資料）管理。\n",
    "\n",
    "### 目錄\n",
    "\n",
    "- [概述](#概述)\n",
    "- [環境設定](#環境設定)\n",
    "- [CharacterTextSplitter 範例](#charactertextsplitter-範例)\n",
    "\n",
    "### 參考資源\n",
    "\n",
    "- [LangChain TextSplitter API 文件](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TextSplitter.html)\n",
    "- [LangChain CharacterTextSplitter API 文件](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- ```langchain-opentutorial``` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [```langchain-opentutorial```](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain_text_splitters\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Adaptive-RAG\",  # title 과 동일하게 설정해 주세요\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as ```OPENAI_API_KEY``` in a ```.env``` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharacterTextSplitter Example\n",
    "\n",
    "Read and store contents from keywords file\n",
    "* Open ```./data/appendix-keywords.txt``` file and read its contents.\n",
    "* Store the read contents in the ```file``` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "   file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 500 characters of the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick access.\n",
      "Related keywords: embedding, database, vectorization, vectorization\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words or sentences, into a low-dimensional, continuous vector. This allows computers to unders\n"
     ]
    }
   ],
   "source": [
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 CharacterTextSplitter 時的參數說明與使用範例\n",
    "\n",
    "在 LangChain 中使用 `CharacterTextSplitter` 可以有效地將長文字分割成更小的區塊，便於後續的語言模型處理。\n",
    "\n",
    "### 📌 參數說明\n",
    "\n",
    "| 參數名稱 | 說明 |\n",
    "|----------|------|\n",
    "| `separator` | 用來分割文字的字串，例如換行符號 `\\n`、空格 `\" \"`、或自訂分隔符。 |\n",
    "| `chunk_size` | 單一文字區塊的最大長度（以字元為單位）。 |\n",
    "| `chunk_overlap` | 相鄰區塊之間重疊的字元數，有助於保留上下文。 |\n",
    "| `length_function` | 用來計算文字長度的函式，預設為 `len`。可自訂長度計算邏輯（例如以 token 數量衡量）。 |\n",
    "| `is_separator_regex` | 設定為 `True` 時，表示 `separator` 是正規表示式（regex）而非純字串。 |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 建立範例\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 建立 TextSplitter 實例\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",                 # 以雙換行分段\n",
    "    chunk_size=300,                  # 每塊最多 300 字元\n",
    "    chunk_overlap=50,               # 每塊重疊 50 字元\n",
    "    length_function=len,            # 使用 Python 內建 len() 計算長度\n",
    "    is_separator_regex=False        # 不使用 regex（純文字分隔）\n",
    ")\n",
    "\n",
    "# 測試文字\n",
    "text = \"\"\"LangChain 是一個強大的框架，可協助開發者構建基於大型語言模型的應用。\n",
    "它提供了模組化的工具與抽象層，讓複雜任務如多輪對話、RAG 等變得更易於實作。\n",
    "\n",
    "TextSplitter 是其中關鍵的組件之一，特別適用於將長文檔切分成小段落。\n",
    "這樣可以更好地讓模型理解上下文，也便於嵌入向量查詢與記憶操作。\"\"\"\n",
    "\n",
    "# 分割結果\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# 輸出分塊\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"=== Chunk {i+1} ===\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "   separator=\" \",           # Splits whenever a space is encountered in text\n",
    "   chunk_size=250,          # Each chunk contains maximum 250 characters\n",
    "   chunk_overlap=50,        # Two consecutive chunks share 50 characters\n",
    "   length_function=len,     # Counts total characters in each chunk\n",
    "   is_separator_regex=False # Uses space as literal separator, not as regex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create document objects from chunks and display the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick'\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.create_documents([file])\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate metadata handling during document creation:\n",
    "\n",
    "* ```create_documents``` accepts both text data and metadata lists\n",
    "* Each chunk inherits metadata from its source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "Definition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\n",
      "Example: Vectors of word embeddings can be stored in a database for quick' metadata={'document': 1}\n"
     ]
    }
   ],
   "source": [
    "# Define metadata for each document\n",
    "metadatas = [\n",
    "   {\"document\": 1},\n",
    "   {\"document\": 2},\n",
    "]\n",
    "\n",
    "# Create documents with metadata\n",
    "documents = text_splitter.create_documents(\n",
    "   [file, file],  # List of texts to split\n",
    "   metadatas=metadatas,  # Corresponding metadata\n",
    ")\n",
    "\n",
    "print(documents[0])  # Display first document with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split text using the ```split_text()``` method.\n",
    "* ```text_splitter.split_text(file)[0]``` returns the first chunk of the split text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Semantic Search\\n\\nDefinition: A vector store is a system that stores data converted to vector format. It is used for search, classification, and other data analysis tasks.\\nExample: Vectors of word embeddings can be stored in a database for quick'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the file text and return the first chunk\n",
    "text_splitter.split_text(file)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-BBeXTkJT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
